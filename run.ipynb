{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ✨️閃開! 人類, 交給AI來!\n",
    "\n",
    "本筆記本主要講述如何使用LLM產生基於特定領域知識的合成資料集\n",
    "\n",
    "- 合成資料？📃  \n",
    "  簡單來講就是用生成式AI來產生的資料. (詳見[What is synthetic data?](https://mostly.ai/what-is-synthetic-data))\n",
    "\n",
    "- 為什麼需要基於特定領域的知識來產生合成資料？🤔  \n",
    "  1. 在企業內部有許多專業領域知識(domain knowledge)都是只有在該領域的專家才懂, 且這些資料大多都不容易閱讀.\n",
    "  2. 透過微調讓LLM可以更貼近特定領域的應用場景, 而要微調便需要先準備好資料."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 運行準備\n",
    "- [Python](https://www.python.org/downloads/release/python-3111/)\n",
    "- [Ollama](https://ollama.com/download)  \n",
    "  本文主要作為示範目的, 所以就只用llama3.1-8b-q4_0的模型來跑(效果已經很不錯了🤩)  \n",
    "  詳見 [How to Run LLM Models Locally with Ollama?](https://www.analyticsvidhya.com/blog/2024/07/local-llm-deployment-with-ollama/)  \n",
    "    \n",
    "  (如果想要使用更大的模型, 但是卻沒有足夠的硬體, 非常推薦使用 [Groq](https://groq.com/)🚀 或是 [Nvidia NIM](https://build.nvidia.com/explore/discover)🌲)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 安裝套件、API key設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install rich unstructured[md] pydantic openai datasets pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 載入文本\n",
    "\n",
    "> 範例文本來源是我將 [Fine-tune Llama 3.1 Ultra-Efficiently with Unsloth](https://huggingface.co/blog/mlabonne/sft-llama3) 翻成中文後的Markdown格式文件. (這篇寫的超讚, 有興趣的話可以讀一讀🦥)\n",
    "\n",
    "因為在實際應用場景中, 文本的長度非常有可能遠超 LLM的上下文長度, 所以必須先對文本做分割處理.  \n",
    "本文只有用 `unstructured` 做簡單的分割, 這部份不是本文的重點就不細談."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rich import print\n",
    "from unstructured.chunking.title import chunk_by_title\n",
    "from unstructured.partition.md import partition_md\n",
    "\n",
    "md_file = './example.md'\n",
    "elements = partition_md(filename=md_file, encoding='utf-8')\n",
    "chunks = chunk_by_title(elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'type'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'CompositeElement'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'element_id'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'11495636-972a-4a66-9f73-2bd56068add6'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'text'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'使用Unsloth對Llama 3.1進行超高效微調\\n\\n原文連結\\n\\nmlabonne Maxime </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Labonne初學者指南：最新的監督微調技術\\n\\nLlama 3.1 </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">的最新發布提供了具有驚人性能的模型，縮小了封閉源代碼和開源模型之間的差距。與其使用像 GPT-4o 和 Claude 3.5 </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">這樣的通用、預先訓練好的語言模型，你可以根據自己的具體需求對 Llama 3.1 </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">進行微調，以實現更好的性能和可定制性，並降低成本。\\n\\n在本文中，我們將提供一份全面性的監督微調概覽。接下來，我們將</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">比較監督微調與提示工程，以了解何時使用監督微調是合理的，詳細介紹主要技術及其優缺點，並介紹重要概念，如LoRA超參數、</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">存儲格式和聊天模板。最後，我們將在實踐中實現監督微調，通過在Google Colab中使用Unsloth對Llama 3.1 </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">8B進行微調，達到最先進的優化效果。本文中使用的所有代碼都可以在Google Colab和LLM Course中找到。特別感謝Daniel </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Han回答我的問題。'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'metadata'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'emphasized_text_contents'</span>: <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'mlabonne Maxime Labonne初學者指南：最新的監督微調技術'</span><span style=\"font-weight: bold\">]</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'emphasized_text_tags'</span>: <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'i'</span><span style=\"font-weight: bold\">]</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'file_directory'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'.'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'filename'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'example.md'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'filetype'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'text/markdown'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'languages'</span>: <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'eng'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'nor'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'cat'</span><span style=\"font-weight: bold\">]</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'last_modified'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'2024-08-01T00:33:19'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'link_texts'</span>: <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'原文連結'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'Google Colab'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'LLM Course'</span><span style=\"font-weight: bold\">]</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'link_urls'</span>: <span style=\"font-weight: bold\">[</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'https://huggingface.co/blog/mlabonne/sft-llama3'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'https://colab.research.google.com/drive/164cg_O7SV7G8kZr_JXqLd6VC7pd86-1Z#scrollTo=PoPKQjga6obN'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'https://github.com/mlabonne/llm-course'</span>\n",
       "        <span style=\"font-weight: bold\">]</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'orig_elements'</span>: \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'eJzVV1tvG1UQ/isr89rE536JxAMUqQhSKBAq1KayznW9dC/BXkNLxX9n5mzW3aSY5gnRl5NkZvzN/Rvn5btValOX+nHTxNVFtWJJJxYV18pq64n0ir</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">pAZXDCWhOZXD2qVl0aXXSjA/t3q+DGVA+7t5uYbsYtiAhY5KZNm9jsUhhBhbjnq1tx77qEgvTGdTdtOu/irBnf3hTNmN6M687tXsfhjx6Vrevrg6vTH</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">rQvV6mvUdgPO/wB7levis1+3HRDbHKTpkQIE2fEnBF6RcgF5xfUrv56NKGj/vogss7XBy2Z+bnft8O4vT7IQOJl6zpX8XN6fbCEs+uDMSLAG40EiYvm</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">+qCkgFdml0DufMZA5uivmrFN6OmDykYqs4Ei8ux5CiaRxJk0wRhvoyfhk64s2jX96w16mMCgPtxmrJXRWEnGodZRkgkTbQ+7djLdjuPN/mK93h7quun</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">r7EI6D8Pat0O97lrnh75P630ez1psDV+9utvHE34e0hORhYB0oqKGy8xydERL5YPjVivj0sN6krqbrds3f6ZY8t+EoR/Bx5TcnED11L1pulRdTn9C2I</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">xGeL2GUTKEwHApTjSmI+HNmTqQaEIwNQ8vBCTwTZigVnw5gGDDDMFRlbwU+H5Io6uncJqi/p9O0bKv/33dHjIwPBFrA2WOeZWEzFL4pGmWmYakrGef9</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">BIvy39kwWqu4N2aaqBEmQjH6keCZGo9vMkokFOti71FwpQO5R67QpjGnuV4xHQMaFZq47FzBDDh+qTCw/mIFghSsTSAppJFX6n00niMQZhQtH7WLjFF</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Eh61ls8eZUzY+xghBk4I0jsr00O1Wp4EkJCQqyfPrs7EUE1+qsetO8QEVZEVUg21xdsRGyRu/jRgY9SGEsTGc2EcLdSUMQqrjzUwzpZpJOSf6iGyQgS</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">eSj0STrvh6FdE8GVocphTpsf8Su3hTGE8U8eCYNNlq5ZNXd625ULcui2upE9QFM0zaJW6H/iynVMbpjCltxgU42q2mTEZSCxEjmkJxGRYHqVZmJrxoB</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">WUhGUBVTRBUUvhK4lkNkVOIuMuB0k/6RX88I4+GYa6TdXjAfgQ1ZeXT+GPw26f/uWSBrQ+36V9cruwPa8LBtzUbh13ze9pTZUI9eZ7/dNz/cS8frHbf</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">PPLb5dRPX+sb6JRZ/TFZ/uwG9r2avj82fDs2x9+rZ0a/Hfof3ZRN+P24Avm8Ua3bXcWbmO7d6M1LsXU6elSwzTEeTJgDnDsSXkDruNdUiFl6cucUWVw</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">sBWbZ+sj9G4NTrn1x3VX3MmCicgZCeZkDB6XyGTCT+NP5DFFqy0vy4ibox3xd3dpIjNjEUdkiYuscDGXlHMyC8VwrwRBG0HUnPWEb5wuxMJLlYIvv1v</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">8nXv0aOjyzOGWBjcTnoRTBvYZI4ctSnd3dYlmaYhHtFJVmbOc7SVILocfv5i/J4MPPA1SH6lQeomkSpF1gMIK65A8M4chAmOyzM4kqDTW5bZnhcjg23</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">M43a1pwibGMomSecKWHHaqvhPmRN+WIDMh2nLzZrSP/c9QmS8/Rq2WSFHokRwTK/dh+tTM4tgWyama/9lQ2ob35bi/RneHqJAzE2S+wct7aSe6XtyTD</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">1PFlrynmdmHYjrNcU9xaKYtShgOM6W4Dp7Gr1zfpLb62vWgUzhS2ksxt+w2PykyXkdxmvhf/Q1u2IIX'</span>\n",
       "    <span style=\"font-weight: bold\">}</span>\n",
       "<span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m{\u001b[0m\n",
       "    \u001b[32m'type'\u001b[0m: \u001b[32m'CompositeElement'\u001b[0m,\n",
       "    \u001b[32m'element_id'\u001b[0m: \u001b[32m'11495636-972a-4a66-9f73-2bd56068add6'\u001b[0m,\n",
       "    \u001b[32m'text'\u001b[0m: \u001b[32m'使用Unsloth對Llama 3.1進行超高效微調\\n\\n原文連結\\n\\nmlabonne Maxime \u001b[0m\n",
       "\u001b[32mLabonne初學者指南：最新的監督微調技術\\n\\nLlama 3.1 \u001b[0m\n",
       "\u001b[32m的最新發布提供了具有驚人性能的模型，縮小了封閉源代碼和開源模型之間的差距。與其使用像 GPT-4o 和 Claude 3.5 \u001b[0m\n",
       "\u001b[32m這樣的通用、預先訓練好的語言模型，你可以根據自己的具體需求對 Llama 3.1 \u001b[0m\n",
       "\u001b[32m進行微調，以實現更好的性能和可定制性，並降低成本。\\n\\n在本文中，我們將提供一份全面性的監督微調概覽。接下來，我們將\u001b[0m\n",
       "\u001b[32m比較監督微調與提示工程，以了解何時使用監督微調是合理的，詳細介紹主要技術及其優缺點，並介紹重要概念，如LoRA超參數、\u001b[0m\n",
       "\u001b[32m存儲格式和聊天模板。最後，我們將在實踐中實現監督微調，通過在Google Colab中使用Unsloth對Llama 3.1 \u001b[0m\n",
       "\u001b[32m8B進行微調，達到最先進的優化效果。本文中使用的所有代碼都可以在Google Colab和LLM Course中找到。特別感謝Daniel \u001b[0m\n",
       "\u001b[32mHan回答我的問題。'\u001b[0m,\n",
       "    \u001b[32m'metadata'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "        \u001b[32m'emphasized_text_contents'\u001b[0m: \u001b[1m[\u001b[0m\u001b[32m'mlabonne Maxime Labonne初學者指南：最新的監督微調技術'\u001b[0m\u001b[1m]\u001b[0m,\n",
       "        \u001b[32m'emphasized_text_tags'\u001b[0m: \u001b[1m[\u001b[0m\u001b[32m'i'\u001b[0m\u001b[1m]\u001b[0m,\n",
       "        \u001b[32m'file_directory'\u001b[0m: \u001b[32m'.'\u001b[0m,\n",
       "        \u001b[32m'filename'\u001b[0m: \u001b[32m'example.md'\u001b[0m,\n",
       "        \u001b[32m'filetype'\u001b[0m: \u001b[32m'text/markdown'\u001b[0m,\n",
       "        \u001b[32m'languages'\u001b[0m: \u001b[1m[\u001b[0m\u001b[32m'eng'\u001b[0m, \u001b[32m'nor'\u001b[0m, \u001b[32m'cat'\u001b[0m\u001b[1m]\u001b[0m,\n",
       "        \u001b[32m'last_modified'\u001b[0m: \u001b[32m'2024-08-01T00:33:19'\u001b[0m,\n",
       "        \u001b[32m'link_texts'\u001b[0m: \u001b[1m[\u001b[0m\u001b[32m'原文連結'\u001b[0m, \u001b[32m'Google Colab'\u001b[0m, \u001b[32m'LLM Course'\u001b[0m\u001b[1m]\u001b[0m,\n",
       "        \u001b[32m'link_urls'\u001b[0m: \u001b[1m[\u001b[0m\n",
       "            \u001b[32m'https://huggingface.co/blog/mlabonne/sft-llama3'\u001b[0m,\n",
       "            \u001b[32m'https://colab.research.google.com/drive/164cg_O7SV7G8kZr_JXqLd6VC7pd86-1Z#\u001b[0m\u001b[32mscrollTo\u001b[0m\u001b[32m=\u001b[0m\u001b[32mPoPKQjga6obN\u001b[0m\u001b[32m'\u001b[0m,\n",
       "            \u001b[32m'https://github.com/mlabonne/llm-course'\u001b[0m\n",
       "        \u001b[1m]\u001b[0m,\n",
       "        \u001b[32m'orig_elements'\u001b[0m: \n",
       "\u001b[32m'eJzVV1tvG1UQ/isr89rE536JxAMUqQhSKBAq1KayznW9dC/BXkNLxX9n5mzW3aSY5gnRl5NkZvzN/Rvn5btValOX+nHTxNVFtWJJJxYV18pq64n0ir\u001b[0m\n",
       "\u001b[32mpAZXDCWhOZXD2qVl0aXXSjA/t3q+DGVA+7t5uYbsYtiAhY5KZNm9jsUhhBhbjnq1tx77qEgvTGdTdtOu/irBnf3hTNmN6M687tXsfhjx6Vrevrg6vTH\u001b[0m\n",
       "\u001b[32mrQvV6mvUdgPO/wB7levis1+3HRDbHKTpkQIE2fEnBF6RcgF5xfUrv56NKGj/vogss7XBy2Z+bnft8O4vT7IQOJl6zpX8XN6fbCEs+uDMSLAG40EiYvm\u001b[0m\n",
       "\u001b[32m+qCkgFdml0DufMZA5uivmrFN6OmDykYqs4Ei8ux5CiaRxJk0wRhvoyfhk64s2jX96w16mMCgPtxmrJXRWEnGodZRkgkTbQ+7djLdjuPN/mK93h7quun\u001b[0m\n",
       "\u001b[32mr7EI6D8Pat0O97lrnh75P630ez1psDV+9utvHE34e0hORhYB0oqKGy8xydERL5YPjVivj0sN6krqbrds3f6ZY8t+EoR/Bx5TcnED11L1pulRdTn9C2I\u001b[0m\n",
       "\u001b[32mxGeL2GUTKEwHApTjSmI+HNmTqQaEIwNQ8vBCTwTZigVnw5gGDDDMFRlbwU+H5Io6uncJqi/p9O0bKv/33dHjIwPBFrA2WOeZWEzFL4pGmWmYakrGef9\u001b[0m\n",
       "\u001b[32mBIvy39kwWqu4N2aaqBEmQjH6keCZGo9vMkokFOti71FwpQO5R67QpjGnuV4xHQMaFZq47FzBDDh+qTCw/mIFghSsTSAppJFX6n00niMQZhQtH7WLjFF\u001b[0m\n",
       "\u001b[32mEh61ls8eZUzY+xghBk4I0jsr00O1Wp4EkJCQqyfPrs7EUE1+qsetO8QEVZEVUg21xdsRGyRu/jRgY9SGEsTGc2EcLdSUMQqrjzUwzpZpJOSf6iGyQgS\u001b[0m\n",
       "\u001b[32meSj0STrvh6FdE8GVocphTpsf8Su3hTGE8U8eCYNNlq5ZNXd625ULcui2upE9QFM0zaJW6H/iynVMbpjCltxgU42q2mTEZSCxEjmkJxGRYHqVZmJrxoB\u001b[0m\n",
       "\u001b[32mWUhGUBVTRBUUvhK4lkNkVOIuMuB0k/6RX88I4+GYa6TdXjAfgQ1ZeXT+GPw26f/uWSBrQ+36V9cruwPa8LBtzUbh13ze9pTZUI9eZ7/dNz/cS8frHbf\u001b[0m\n",
       "\u001b[32mPPLb5dRPX+sb6JRZ/TFZ/uwG9r2avj82fDs2x9+rZ0a/Hfof3ZRN+P24Avm8Ua3bXcWbmO7d6M1LsXU6elSwzTEeTJgDnDsSXkDruNdUiFl6cucUWVw\u001b[0m\n",
       "\u001b[32msBWbZ+sj9G4NTrn1x3VX3MmCicgZCeZkDB6XyGTCT+NP5DFFqy0vy4ibox3xd3dpIjNjEUdkiYuscDGXlHMyC8VwrwRBG0HUnPWEb5wuxMJLlYIvv1v\u001b[0m\n",
       "\u001b[32m8nXv0aOjyzOGWBjcTnoRTBvYZI4ctSnd3dYlmaYhHtFJVmbOc7SVILocfv5i/J4MPPA1SH6lQeomkSpF1gMIK65A8M4chAmOyzM4kqDTW5bZnhcjg23\u001b[0m\n",
       "\u001b[32mM43a1pwibGMomSecKWHHaqvhPmRN+WIDMh2nLzZrSP/c9QmS8/Rq2WSFHokRwTK/dh+tTM4tgWyama/9lQ2ob35bi/RneHqJAzE2S+wct7aSe6XtyTD\u001b[0m\n",
       "\u001b[32m1PFlrynmdmHYjrNcU9xaKYtShgOM6W4Dp7Gr1zfpLb62vWgUzhS2ksxt+w2PykyXkdxmvhf/Q1u2IIX'\u001b[0m\n",
       "    \u001b[1m}\u001b[0m\n",
       "\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(chunks[0].to_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 來產生吧 🎏\n",
    "\n",
    "為了盡量精簡, 這邊示範的是產生Alpaca格式的資料集, 每筆資料中只有單一輪的問答.  \n",
    "如果想產生其他的格式, 只要照著這個思路在做擴充即可!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 系統提示詞, 提示詞範本\n",
    "\n",
    "透過系統提示詞先跟LLM講好要做哪些事情.\n",
    "\n",
    "透過題實詞範本來替換不同的文本片段."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "system = (\n",
    "    \"\"\"你是一個非常有經驗的文章關鍵點提取者,可以精準的從一段文章中找出關鍵重點,並依據這些關鍵點來生成一般人在閱讀這段文章時可能會提出的問題或是想要進一部理解的部份,可以是複合式的問題或各個關鍵點之間的關聯性等, 問題的描述寫的越仔細越好, 你的目標是讓使用者可以在學會這些問題後就能理解整篇文章.\n",
    "你會依據使用者提供的文章來生成一個資料集,資料集格式如下, 在\"Instruction\"中填入你設計的問題:\n",
    "```\n",
    "[\n",
    "    {\"Instruction\": \"\"},\n",
    "    {\"Instruction\": \"\"},\n",
    "    {\"Instruction\": \"\"},\n",
    "]\n",
    "```\n",
    "你生成的資料集必須盡可能涵蓋整篇文章所提及的內容.\n",
    "回覆使用者時只需要提供你所生成的資料集,不須做任何額外的說明. 一律使用繁體中文.\"\"\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template =(\n",
    "    \"\"\"文章內容:\n",
    "```\n",
    "{content}\n",
    "```\n",
    "'請依據此文章生成至少 {amount} 筆資料\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Few-shot prompting\n",
    "\n",
    "透過 [Few-shot prompting](https://arxiv.org/abs/2205.05638) 的方式預先定好幾輪對話, 可以非常有效的提生LLM按照自己想要的方式來輸出的機率!  \n",
    "\n",
    "我自己在測試過程中透過Few-shot prompting的方式就連7b, 8b等相對較小都能有非常穩定的輸出."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "incontext_user = (\n",
    "    \"\"\"文章內容:\n",
    "```\n",
    "1. 第一步 - 模型載入：將模型參數移到GPU上。目前的記憶體：模型。\n",
    "2. 第二步 - 前向傳遞：將輸入通過模型，並存儲中間輸出（激活值）。在這一步驟中，存儲這些激活值佔用了記憶體。雖然存儲所有激活值並不是絕對必要的（參見「梯度檢查點」），但對於反向傳播算法的效率而言，存儲它們實際上是必要的。目前的記憶體：模型 +  激活值。\n",
    "3. 第三步 - 反向傳播：從網絡末端計算梯度，並在計算的過程中捨棄前向傳遞的激活值。由於我們已經捨棄了前向傳遞的激活值，在反向傳播過程後，記憶體使用量為模型大小的兩倍 - 一份是權重的拷貝，另一份是梯度的拷貝。目前的記憶體：模型 + 梯度。\n",
    "4. 第四步 - 優化器更新：更新參數，並追蹤優化器的運行參數。許多優化器會追蹤每個模型權重的梯度一階和二階矩的估計值。對於Adam（使用兩個矩），它佔用模型大小的兩倍；對於RMSprop（使用一個矩），它佔用模型大小的一倍；對於SGD（不使用矩），則不佔用模型大小。目前的記憶體：模型 + 梯度 + 梯度矩。\n",
    "5. 第五步 - 執行下一次迭代：在計算了一次梯度並且優化器進行了一步後，梯度和梯度矩仍然存在。因此，在未來的迭代中，您的總最大記憶體使用量將為：模型 + 激活值 + 梯度 + 梯度矩，這意味著記憶體使用量在第二次迭代時會增加，但之後將保持不變。\n",
    "\n",
    "現在，對於通常會消耗記憶體的基本了解後，讓我們來看一些特殊情況和優化方法來節省記憶體。\n",
    "```\n",
    "請依據此文章生成至少 5 筆資料。\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "incontext_assistant = (\n",
    "    \"\"\"[\n",
    "{\"Instruction\": \"什麼是模型載入的第一步驟？\"},\n",
    "{\"Instruction\": \"前向傳遞的過程中，為什麼需要存儲中間輸出（激活值）？\"},\n",
    "{\"Instruction\": \"在反向傳播過程中，為什麼需要捨棄前向傳遞的激活值？\"},\n",
    "{\"Instruction\": \"優化器更新的過程中，什麼是梯度一階和二階矩的估計值？\"},\n",
    "{\"Instruction\": \"在執行下一次迭代時，為什麼梯度和梯度矩仍然存在？\"},\n",
    "{\"Instruction\": \"什麼是梯度檢查點？\"},\n",
    "{\"Instruction\": \"什麼是Adam、RMSprop和SGD優化器的差異？\"},\n",
    "{\"Instruction\": \"如何計算模型的記憶體使用量？\"}\n",
    "]\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由於每個文本片段的長短不一, 實際應用中可以透過Tokenizer來計算每個文本片段的token數量再乘上一個比例來控制對於該文本片段而言我們要產生幾筆合成資料.\n",
    "\n",
    "本文為了盡量精簡, 就直接計算文本片段的字數來決定要產生幾筆合成資料 👍️"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def count_words(text):\n",
    "    # 計算中文字符\n",
    "    chinese_chars = len(re.findall(r'[\\u4e00-\\u9fff]', text))\n",
    "    \n",
    "    # 移除所有中文字符和標點符號，只保留英文和數字\n",
    "    english_text = re.sub(r'[\\u4e00-\\u9fff]|[^\\w\\s]', ' ', text)\n",
    "    \n",
    "    # 分割英文單詞並計數\n",
    "    english_words = len(english_text.split())\n",
    "    \n",
    "    # 總字數為中文字符數加上英文單詞數\n",
    "    total_words = chinese_chars + english_words\n",
    "    \n",
    "    return total_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用 `pydantic` 來驗證和解析生成的資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Any, Dict\n",
    "from pydantic import BaseModel, Json, ValidationError\n",
    "\n",
    "class SyntheticData(BaseModel):\n",
    "    Instruction: str = \"\"\n",
    "    Input: str = \"\"\n",
    "    Output: str = \"\"\n",
    "    Metadata: Optional[Dict[str, Any]]\n",
    "\n",
    "\n",
    "class VaildResponse(BaseModel):\n",
    "    vaild_question: Json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Generate instruction <span style=\"color: #808000; text-decoration-color: #808000\">count</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">81</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Generate instruction \u001b[33mcount\u001b[0m=\u001b[1;36m81\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">SyntheticData</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">Instruction</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'什麼是監督微調（SFT）的局限性？'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">Input</span>=<span style=\"color: #008000; text-decoration-color: #008000\">''</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">Output</span>=<span style=\"color: #008000; text-decoration-color: #008000\">''</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">Metadata</span>=<span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'source'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'./example.md'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'content'</span>: \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'然而，監督微調（SFT）也有其局限性。它在利用基礎模型已有的知識時效果最佳。學習完全新的信息，例如未知語言，可能會很</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">具挑戰性，並導致更頻繁的幻覺。對於基礎模型未知的新領域，建議先在原始數據集上進行持續的預訓練。在另一端，指令模型（</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">即已經微調過的模型）可能已經非常接近您的需求。例如，一個模型可能表現得非常好，但聲稱它是由OpenAI或Meta訓練的，而不</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">是您。在這種情況下，您可能希望通過偏好對齊（preference </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">alignment）稍微調整指令模型的行為。通過提供選擇和拒絕的樣本（約100到1000個樣本）來微調模型的行為，您可以強迫LLM說明</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">您訓練了模型，而不是OpenAI。\\n\\n。⚖️ 監督微調技術（SFT </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Techniques）\\n\\n目前最受歡迎的三種監督微調技術是完整微調（full fine-tuning）、LoRA和QLoRA。'</span>\n",
       "        <span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">SyntheticData</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">Instruction</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'在利用基礎模型已有的知識時，什麼情況下監督微調效果最佳？'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">Input</span>=<span style=\"color: #008000; text-decoration-color: #008000\">''</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">Output</span>=<span style=\"color: #008000; text-decoration-color: #008000\">''</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">Metadata</span>=<span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'source'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'./example.md'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'content'</span>: \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'然而，監督微調（SFT）也有其局限性。它在利用基礎模型已有的知識時效果最佳。學習完全新的信息，例如未知語言，可能會很</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">具挑戰性，並導致更頻繁的幻覺。對於基礎模型未知的新領域，建議先在原始數據集上進行持續的預訓練。在另一端，指令模型（</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">即已經微調過的模型）可能已經非常接近您的需求。例如，一個模型可能表現得非常好，但聲稱它是由OpenAI或Meta訓練的，而不</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">是您。在這種情況下，您可能希望通過偏好對齊（preference </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">alignment）稍微調整指令模型的行為。通過提供選擇和拒絕的樣本（約100到1000個樣本）來微調模型的行為，您可以強迫LLM說明</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">您訓練了模型，而不是OpenAI。\\n\\n。⚖️ 監督微調技術（SFT </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Techniques）\\n\\n目前最受歡迎的三種監督微調技術是完整微調（full fine-tuning）、LoRA和QLoRA。'</span>\n",
       "        <span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">SyntheticData</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">Instruction</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'如何使用偏好對齊（preference alignment）來微調指令模型的行為？'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">Input</span>=<span style=\"color: #008000; text-decoration-color: #008000\">''</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">Output</span>=<span style=\"color: #008000; text-decoration-color: #008000\">''</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">Metadata</span>=<span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'source'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'./example.md'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'content'</span>: \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'然而，監督微調（SFT）也有其局限性。它在利用基礎模型已有的知識時效果最佳。學習完全新的信息，例如未知語言，可能會很</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">具挑戰性，並導致更頻繁的幻覺。對於基礎模型未知的新領域，建議先在原始數據集上進行持續的預訓練。在另一端，指令模型（</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">即已經微調過的模型）可能已經非常接近您的需求。例如，一個模型可能表現得非常好，但聲稱它是由OpenAI或Meta訓練的，而不</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">是您。在這種情況下，您可能希望通過偏好對齊（preference </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">alignment）稍微調整指令模型的行為。通過提供選擇和拒絕的樣本（約100到1000個樣本）來微調模型的行為，您可以強迫LLM說明</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">您訓練了模型，而不是OpenAI。\\n\\n。⚖️ 監督微調技術（SFT </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Techniques）\\n\\n目前最受歡迎的三種監督微調技術是完整微調（full fine-tuning）、LoRA和QLoRA。'</span>\n",
       "        <span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">SyntheticData</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">Instruction</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'什麼是完整微調（Full fine-tuning）？'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">Input</span>=<span style=\"color: #008000; text-decoration-color: #008000\">''</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">Output</span>=<span style=\"color: #008000; text-decoration-color: #008000\">''</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">Metadata</span>=<span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'source'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'./example.md'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'content'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'完整微調（Full </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">fine-tuning）是最直接的監督微調技術。它涉及使用指令數據集重新訓練預先訓練模型的所有參數。這種方法通常能夠提供最佳結</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">果，但需要大量的計算資源（需要多個高端GPU來微調一個8B模型）。由於它修改了整個模型，因此也是最具破壞性的方法，可能會</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">導致之前學習的技能和知識的災難性遺忘。\\n\\n低秩適應（Low-Rank </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Adaptation，LoRA）是一種流行的參數高效微調技術。與其重新訓練整個模型，LoRA會凍結權重並在每個目標層引入小型適配器（</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">低秩矩陣）。這使得LoRA能夠訓練的參數數量遠遠低於完整的微調（少於1％），從而減少了內存使用和訓練時間。這種方法是非破</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">壞性的，因為原始參數被凍結，適配器可以在後續被切換或組合。'</span>\n",
       "        <span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">SyntheticData</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">Instruction</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'完整微調的優點和缺點是什麼？'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">Input</span>=<span style=\"color: #008000; text-decoration-color: #008000\">''</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">Output</span>=<span style=\"color: #008000; text-decoration-color: #008000\">''</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">Metadata</span>=<span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'source'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'./example.md'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'content'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'完整微調（Full </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">fine-tuning）是最直接的監督微調技術。它涉及使用指令數據集重新訓練預先訓練模型的所有參數。這種方法通常能夠提供最佳結</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">果，但需要大量的計算資源（需要多個高端GPU來微調一個8B模型）。由於它修改了整個模型，因此也是最具破壞性的方法，可能會</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">導致之前學習的技能和知識的災難性遺忘。\\n\\n低秩適應（Low-Rank </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Adaptation，LoRA）是一種流行的參數高效微調技術。與其重新訓練整個模型，LoRA會凍結權重並在每個目標層引入小型適配器（</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">低秩矩陣）。這使得LoRA能夠訓練的參數數量遠遠低於完整的微調（少於1％），從而減少了內存使用和訓練時間。這種方法是非破</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">壞性的，因為原始參數被凍結，適配器可以在後續被切換或組合。'</span>\n",
       "        <span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">SyntheticData</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">Instruction</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'低秩適應（Low-Rank Adaptation，LoRA）的工作原理是什麼？'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">Input</span>=<span style=\"color: #008000; text-decoration-color: #008000\">''</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">Output</span>=<span style=\"color: #008000; text-decoration-color: #008000\">''</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">Metadata</span>=<span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'source'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'./example.md'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'content'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'完整微調（Full </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">fine-tuning）是最直接的監督微調技術。它涉及使用指令數據集重新訓練預先訓練模型的所有參數。這種方法通常能夠提供最佳結</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">果，但需要大量的計算資源（需要多個高端GPU來微調一個8B模型）。由於它修改了整個模型，因此也是最具破壞性的方法，可能會</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">導致之前學習的技能和知識的災難性遺忘。\\n\\n低秩適應（Low-Rank </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Adaptation，LoRA）是一種流行的參數高效微調技術。與其重新訓練整個模型，LoRA會凍結權重並在每個目標層引入小型適配器（</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">低秩矩陣）。這使得LoRA能夠訓練的參數數量遠遠低於完整的微調（少於1％），從而減少了內存使用和訓練時間。這種方法是非破</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">壞性的，因為原始參數被凍結，適配器可以在後續被切換或組合。'</span>\n",
       "        <span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">SyntheticData</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">Instruction</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'與完整微調相比，低秩適應的計算資源需求如何？'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">Input</span>=<span style=\"color: #008000; text-decoration-color: #008000\">''</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">Output</span>=<span style=\"color: #008000; text-decoration-color: #008000\">''</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">Metadata</span>=<span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'source'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'./example.md'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'content'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'完整微調（Full </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">fine-tuning）是最直接的監督微調技術。它涉及使用指令數據集重新訓練預先訓練模型的所有參數。這種方法通常能夠提供最佳結</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">果，但需要大量的計算資源（需要多個高端GPU來微調一個8B模型）。由於它修改了整個模型，因此也是最具破壞性的方法，可能會</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">導致之前學習的技能和知識的災難性遺忘。\\n\\n低秩適應（Low-Rank </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Adaptation，LoRA）是一種流行的參數高效微調技術。與其重新訓練整個模型，LoRA會凍結權重並在每個目標層引入小型適配器（</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">低秩矩陣）。這使得LoRA能夠訓練的參數數量遠遠低於完整的微調（少於1％），從而減少了內存使用和訓練時間。這種方法是非破</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">壞性的，因為原始參數被凍結，適配器可以在後續被切換或組合。'</span>\n",
       "        <span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">SyntheticData</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">Instruction</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'低秩適應是否是一種破壞性方法？'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">Input</span>=<span style=\"color: #008000; text-decoration-color: #008000\">''</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">Output</span>=<span style=\"color: #008000; text-decoration-color: #008000\">''</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">Metadata</span>=<span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'source'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'./example.md'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'content'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'完整微調（Full </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">fine-tuning）是最直接的監督微調技術。它涉及使用指令數據集重新訓練預先訓練模型的所有參數。這種方法通常能夠提供最佳結</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">果，但需要大量的計算資源（需要多個高端GPU來微調一個8B模型）。由於它修改了整個模型，因此也是最具破壞性的方法，可能會</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">導致之前學習的技能和知識的災難性遺忘。\\n\\n低秩適應（Low-Rank </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Adaptation，LoRA）是一種流行的參數高效微調技術。與其重新訓練整個模型，LoRA會凍結權重並在每個目標層引入小型適配器（</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">低秩矩陣）。這使得LoRA能夠訓練的參數數量遠遠低於完整的微調（少於1％），從而減少了內存使用和訓練時間。這種方法是非破</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">壞性的，因為原始參數被凍結，適配器可以在後續被切換或組合。'</span>\n",
       "        <span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">SyntheticData</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">Instruction</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'完整微調和低秩適應哪種方法能夠提供最佳結果？'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">Input</span>=<span style=\"color: #008000; text-decoration-color: #008000\">''</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">Output</span>=<span style=\"color: #008000; text-decoration-color: #008000\">''</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">Metadata</span>=<span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'source'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'./example.md'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'content'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'完整微調（Full </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">fine-tuning）是最直接的監督微調技術。它涉及使用指令數據集重新訓練預先訓練模型的所有參數。這種方法通常能夠提供最佳結</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">果，但需要大量的計算資源（需要多個高端GPU來微調一個8B模型）。由於它修改了整個模型，因此也是最具破壞性的方法，可能會</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">導致之前學習的技能和知識的災難性遺忘。\\n\\n低秩適應（Low-Rank </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Adaptation，LoRA）是一種流行的參數高效微調技術。與其重新訓練整個模型，LoRA會凍結權重並在每個目標層引入小型適配器（</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">低秩矩陣）。這使得LoRA能夠訓練的參數數量遠遠低於完整的微調（少於1％），從而減少了內存使用和訓練時間。這種方法是非破</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">壞性的，因為原始參數被凍結，適配器可以在後續被切換或組合。'</span>\n",
       "        <span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">SyntheticData</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">Instruction</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'什麼是QLoRA，它提供了哪些優勢？'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">Input</span>=<span style=\"color: #008000; text-decoration-color: #008000\">''</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">Output</span>=<span style=\"color: #008000; text-decoration-color: #008000\">''</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">Metadata</span>=<span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'source'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'./example.md'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'content'</span>: \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'QLoRA（量化感知低秩適應）是LoRA的一個擴展，提供了更大的記憶體節省。它提供了高達33%的額外記憶體節省，相比於標準的Lo</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">RA，特別是在GPU記憶體受限的情況下。這種增加的效率是以訓練時間更長為代價的，QLoRA通常需要比標準LoRA多39%的時間來訓練</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">。雖然QLoRA需要更多的訓練時間，但它在記憶體方面的顯著節省可以使其成為在GPU記憶體有限的場景中唯一可行的選擇。因此，</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">這就是我們在下一節中將用來對Llama 3.1 8B模型在Google Colab上進行微調的技術。\\n\\n🦙 對 Llama 3.1 8B </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">進行微調\\n\\n為了有效地對 Llama 3.1 8B 模型進行微調，我們將使用 Daniel 和 Michael Han 開發的 Unsloth </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">庫。由於其自定義核心，Unsloth 提供了 2 倍的訓練速度和 60% 的記憶體使用率，相比其他選擇更適合在 Colab </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">等受限環境中使用。不幸的是，Unsloth 目前只支持單 GPU 設置。對於多 GPU 設置，我推薦流行的替代方案，如 TRL 和 </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Axolotl（兩者都將 Unsloth 作為後端）。'</span>\n",
       "        <span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\n",
       "    \u001b[1;35mSyntheticData\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mInstruction\u001b[0m=\u001b[32m'什麼是監督微調（SFT）的局限性？'\u001b[0m,\n",
       "        \u001b[33mInput\u001b[0m=\u001b[32m''\u001b[0m,\n",
       "        \u001b[33mOutput\u001b[0m=\u001b[32m''\u001b[0m,\n",
       "        \u001b[33mMetadata\u001b[0m=\u001b[1m{\u001b[0m\n",
       "            \u001b[32m'source'\u001b[0m: \u001b[32m'./example.md'\u001b[0m,\n",
       "            \u001b[32m'content'\u001b[0m: \n",
       "\u001b[32m'然而，監督微調（SFT）也有其局限性。它在利用基礎模型已有的知識時效果最佳。學習完全新的信息，例如未知語言，可能會很\u001b[0m\n",
       "\u001b[32m具挑戰性，並導致更頻繁的幻覺。對於基礎模型未知的新領域，建議先在原始數據集上進行持續的預訓練。在另一端，指令模型（\u001b[0m\n",
       "\u001b[32m即已經微調過的模型）可能已經非常接近您的需求。例如，一個模型可能表現得非常好，但聲稱它是由OpenAI或Meta訓練的，而不\u001b[0m\n",
       "\u001b[32m是您。在這種情況下，您可能希望通過偏好對齊（preference \u001b[0m\n",
       "\u001b[32malignment）稍微調整指令模型的行為。通過提供選擇和拒絕的樣本（約100到1000個樣本）來微調模型的行為，您可以強迫LLM說明\u001b[0m\n",
       "\u001b[32m您訓練了模型，而不是OpenAI。\\n\\n。⚖️ 監督微調技術（SFT \u001b[0m\n",
       "\u001b[32mTechniques）\\n\\n目前最受歡迎的三種監督微調技術是完整微調（full fine-tuning）、LoRA和QLoRA。'\u001b[0m\n",
       "        \u001b[1m}\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mSyntheticData\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mInstruction\u001b[0m=\u001b[32m'在利用基礎模型已有的知識時，什麼情況下監督微調效果最佳？'\u001b[0m,\n",
       "        \u001b[33mInput\u001b[0m=\u001b[32m''\u001b[0m,\n",
       "        \u001b[33mOutput\u001b[0m=\u001b[32m''\u001b[0m,\n",
       "        \u001b[33mMetadata\u001b[0m=\u001b[1m{\u001b[0m\n",
       "            \u001b[32m'source'\u001b[0m: \u001b[32m'./example.md'\u001b[0m,\n",
       "            \u001b[32m'content'\u001b[0m: \n",
       "\u001b[32m'然而，監督微調（SFT）也有其局限性。它在利用基礎模型已有的知識時效果最佳。學習完全新的信息，例如未知語言，可能會很\u001b[0m\n",
       "\u001b[32m具挑戰性，並導致更頻繁的幻覺。對於基礎模型未知的新領域，建議先在原始數據集上進行持續的預訓練。在另一端，指令模型（\u001b[0m\n",
       "\u001b[32m即已經微調過的模型）可能已經非常接近您的需求。例如，一個模型可能表現得非常好，但聲稱它是由OpenAI或Meta訓練的，而不\u001b[0m\n",
       "\u001b[32m是您。在這種情況下，您可能希望通過偏好對齊（preference \u001b[0m\n",
       "\u001b[32malignment）稍微調整指令模型的行為。通過提供選擇和拒絕的樣本（約100到1000個樣本）來微調模型的行為，您可以強迫LLM說明\u001b[0m\n",
       "\u001b[32m您訓練了模型，而不是OpenAI。\\n\\n。⚖️ 監督微調技術（SFT \u001b[0m\n",
       "\u001b[32mTechniques）\\n\\n目前最受歡迎的三種監督微調技術是完整微調（full fine-tuning）、LoRA和QLoRA。'\u001b[0m\n",
       "        \u001b[1m}\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mSyntheticData\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mInstruction\u001b[0m=\u001b[32m'如何使用偏好對齊（preference alignment）來微調指令模型的行為？'\u001b[0m,\n",
       "        \u001b[33mInput\u001b[0m=\u001b[32m''\u001b[0m,\n",
       "        \u001b[33mOutput\u001b[0m=\u001b[32m''\u001b[0m,\n",
       "        \u001b[33mMetadata\u001b[0m=\u001b[1m{\u001b[0m\n",
       "            \u001b[32m'source'\u001b[0m: \u001b[32m'./example.md'\u001b[0m,\n",
       "            \u001b[32m'content'\u001b[0m: \n",
       "\u001b[32m'然而，監督微調（SFT）也有其局限性。它在利用基礎模型已有的知識時效果最佳。學習完全新的信息，例如未知語言，可能會很\u001b[0m\n",
       "\u001b[32m具挑戰性，並導致更頻繁的幻覺。對於基礎模型未知的新領域，建議先在原始數據集上進行持續的預訓練。在另一端，指令模型（\u001b[0m\n",
       "\u001b[32m即已經微調過的模型）可能已經非常接近您的需求。例如，一個模型可能表現得非常好，但聲稱它是由OpenAI或Meta訓練的，而不\u001b[0m\n",
       "\u001b[32m是您。在這種情況下，您可能希望通過偏好對齊（preference \u001b[0m\n",
       "\u001b[32malignment）稍微調整指令模型的行為。通過提供選擇和拒絕的樣本（約100到1000個樣本）來微調模型的行為，您可以強迫LLM說明\u001b[0m\n",
       "\u001b[32m您訓練了模型，而不是OpenAI。\\n\\n。⚖️ 監督微調技術（SFT \u001b[0m\n",
       "\u001b[32mTechniques）\\n\\n目前最受歡迎的三種監督微調技術是完整微調（full fine-tuning）、LoRA和QLoRA。'\u001b[0m\n",
       "        \u001b[1m}\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mSyntheticData\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mInstruction\u001b[0m=\u001b[32m'什麼是完整微調（Full fine-tuning）？'\u001b[0m,\n",
       "        \u001b[33mInput\u001b[0m=\u001b[32m''\u001b[0m,\n",
       "        \u001b[33mOutput\u001b[0m=\u001b[32m''\u001b[0m,\n",
       "        \u001b[33mMetadata\u001b[0m=\u001b[1m{\u001b[0m\n",
       "            \u001b[32m'source'\u001b[0m: \u001b[32m'./example.md'\u001b[0m,\n",
       "            \u001b[32m'content'\u001b[0m: \u001b[32m'完整微調（Full \u001b[0m\n",
       "\u001b[32mfine-tuning）是最直接的監督微調技術。它涉及使用指令數據集重新訓練預先訓練模型的所有參數。這種方法通常能夠提供最佳結\u001b[0m\n",
       "\u001b[32m果，但需要大量的計算資源（需要多個高端GPU來微調一個8B模型）。由於它修改了整個模型，因此也是最具破壞性的方法，可能會\u001b[0m\n",
       "\u001b[32m導致之前學習的技能和知識的災難性遺忘。\\n\\n低秩適應（Low-Rank \u001b[0m\n",
       "\u001b[32mAdaptation，LoRA）是一種流行的參數高效微調技術。與其重新訓練整個模型，LoRA會凍結權重並在每個目標層引入小型適配器（\u001b[0m\n",
       "\u001b[32m低秩矩陣）。這使得LoRA能夠訓練的參數數量遠遠低於完整的微調（少於1％），從而減少了內存使用和訓練時間。這種方法是非破\u001b[0m\n",
       "\u001b[32m壞性的，因為原始參數被凍結，適配器可以在後續被切換或組合。'\u001b[0m\n",
       "        \u001b[1m}\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mSyntheticData\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mInstruction\u001b[0m=\u001b[32m'完整微調的優點和缺點是什麼？'\u001b[0m,\n",
       "        \u001b[33mInput\u001b[0m=\u001b[32m''\u001b[0m,\n",
       "        \u001b[33mOutput\u001b[0m=\u001b[32m''\u001b[0m,\n",
       "        \u001b[33mMetadata\u001b[0m=\u001b[1m{\u001b[0m\n",
       "            \u001b[32m'source'\u001b[0m: \u001b[32m'./example.md'\u001b[0m,\n",
       "            \u001b[32m'content'\u001b[0m: \u001b[32m'完整微調（Full \u001b[0m\n",
       "\u001b[32mfine-tuning）是最直接的監督微調技術。它涉及使用指令數據集重新訓練預先訓練模型的所有參數。這種方法通常能夠提供最佳結\u001b[0m\n",
       "\u001b[32m果，但需要大量的計算資源（需要多個高端GPU來微調一個8B模型）。由於它修改了整個模型，因此也是最具破壞性的方法，可能會\u001b[0m\n",
       "\u001b[32m導致之前學習的技能和知識的災難性遺忘。\\n\\n低秩適應（Low-Rank \u001b[0m\n",
       "\u001b[32mAdaptation，LoRA）是一種流行的參數高效微調技術。與其重新訓練整個模型，LoRA會凍結權重並在每個目標層引入小型適配器（\u001b[0m\n",
       "\u001b[32m低秩矩陣）。這使得LoRA能夠訓練的參數數量遠遠低於完整的微調（少於1％），從而減少了內存使用和訓練時間。這種方法是非破\u001b[0m\n",
       "\u001b[32m壞性的，因為原始參數被凍結，適配器可以在後續被切換或組合。'\u001b[0m\n",
       "        \u001b[1m}\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mSyntheticData\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mInstruction\u001b[0m=\u001b[32m'低秩適應（Low-Rank Adaptation，LoRA）的工作原理是什麼？'\u001b[0m,\n",
       "        \u001b[33mInput\u001b[0m=\u001b[32m''\u001b[0m,\n",
       "        \u001b[33mOutput\u001b[0m=\u001b[32m''\u001b[0m,\n",
       "        \u001b[33mMetadata\u001b[0m=\u001b[1m{\u001b[0m\n",
       "            \u001b[32m'source'\u001b[0m: \u001b[32m'./example.md'\u001b[0m,\n",
       "            \u001b[32m'content'\u001b[0m: \u001b[32m'完整微調（Full \u001b[0m\n",
       "\u001b[32mfine-tuning）是最直接的監督微調技術。它涉及使用指令數據集重新訓練預先訓練模型的所有參數。這種方法通常能夠提供最佳結\u001b[0m\n",
       "\u001b[32m果，但需要大量的計算資源（需要多個高端GPU來微調一個8B模型）。由於它修改了整個模型，因此也是最具破壞性的方法，可能會\u001b[0m\n",
       "\u001b[32m導致之前學習的技能和知識的災難性遺忘。\\n\\n低秩適應（Low-Rank \u001b[0m\n",
       "\u001b[32mAdaptation，LoRA）是一種流行的參數高效微調技術。與其重新訓練整個模型，LoRA會凍結權重並在每個目標層引入小型適配器（\u001b[0m\n",
       "\u001b[32m低秩矩陣）。這使得LoRA能夠訓練的參數數量遠遠低於完整的微調（少於1％），從而減少了內存使用和訓練時間。這種方法是非破\u001b[0m\n",
       "\u001b[32m壞性的，因為原始參數被凍結，適配器可以在後續被切換或組合。'\u001b[0m\n",
       "        \u001b[1m}\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mSyntheticData\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mInstruction\u001b[0m=\u001b[32m'與完整微調相比，低秩適應的計算資源需求如何？'\u001b[0m,\n",
       "        \u001b[33mInput\u001b[0m=\u001b[32m''\u001b[0m,\n",
       "        \u001b[33mOutput\u001b[0m=\u001b[32m''\u001b[0m,\n",
       "        \u001b[33mMetadata\u001b[0m=\u001b[1m{\u001b[0m\n",
       "            \u001b[32m'source'\u001b[0m: \u001b[32m'./example.md'\u001b[0m,\n",
       "            \u001b[32m'content'\u001b[0m: \u001b[32m'完整微調（Full \u001b[0m\n",
       "\u001b[32mfine-tuning）是最直接的監督微調技術。它涉及使用指令數據集重新訓練預先訓練模型的所有參數。這種方法通常能夠提供最佳結\u001b[0m\n",
       "\u001b[32m果，但需要大量的計算資源（需要多個高端GPU來微調一個8B模型）。由於它修改了整個模型，因此也是最具破壞性的方法，可能會\u001b[0m\n",
       "\u001b[32m導致之前學習的技能和知識的災難性遺忘。\\n\\n低秩適應（Low-Rank \u001b[0m\n",
       "\u001b[32mAdaptation，LoRA）是一種流行的參數高效微調技術。與其重新訓練整個模型，LoRA會凍結權重並在每個目標層引入小型適配器（\u001b[0m\n",
       "\u001b[32m低秩矩陣）。這使得LoRA能夠訓練的參數數量遠遠低於完整的微調（少於1％），從而減少了內存使用和訓練時間。這種方法是非破\u001b[0m\n",
       "\u001b[32m壞性的，因為原始參數被凍結，適配器可以在後續被切換或組合。'\u001b[0m\n",
       "        \u001b[1m}\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mSyntheticData\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mInstruction\u001b[0m=\u001b[32m'低秩適應是否是一種破壞性方法？'\u001b[0m,\n",
       "        \u001b[33mInput\u001b[0m=\u001b[32m''\u001b[0m,\n",
       "        \u001b[33mOutput\u001b[0m=\u001b[32m''\u001b[0m,\n",
       "        \u001b[33mMetadata\u001b[0m=\u001b[1m{\u001b[0m\n",
       "            \u001b[32m'source'\u001b[0m: \u001b[32m'./example.md'\u001b[0m,\n",
       "            \u001b[32m'content'\u001b[0m: \u001b[32m'完整微調（Full \u001b[0m\n",
       "\u001b[32mfine-tuning）是最直接的監督微調技術。它涉及使用指令數據集重新訓練預先訓練模型的所有參數。這種方法通常能夠提供最佳結\u001b[0m\n",
       "\u001b[32m果，但需要大量的計算資源（需要多個高端GPU來微調一個8B模型）。由於它修改了整個模型，因此也是最具破壞性的方法，可能會\u001b[0m\n",
       "\u001b[32m導致之前學習的技能和知識的災難性遺忘。\\n\\n低秩適應（Low-Rank \u001b[0m\n",
       "\u001b[32mAdaptation，LoRA）是一種流行的參數高效微調技術。與其重新訓練整個模型，LoRA會凍結權重並在每個目標層引入小型適配器（\u001b[0m\n",
       "\u001b[32m低秩矩陣）。這使得LoRA能夠訓練的參數數量遠遠低於完整的微調（少於1％），從而減少了內存使用和訓練時間。這種方法是非破\u001b[0m\n",
       "\u001b[32m壞性的，因為原始參數被凍結，適配器可以在後續被切換或組合。'\u001b[0m\n",
       "        \u001b[1m}\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mSyntheticData\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mInstruction\u001b[0m=\u001b[32m'完整微調和低秩適應哪種方法能夠提供最佳結果？'\u001b[0m,\n",
       "        \u001b[33mInput\u001b[0m=\u001b[32m''\u001b[0m,\n",
       "        \u001b[33mOutput\u001b[0m=\u001b[32m''\u001b[0m,\n",
       "        \u001b[33mMetadata\u001b[0m=\u001b[1m{\u001b[0m\n",
       "            \u001b[32m'source'\u001b[0m: \u001b[32m'./example.md'\u001b[0m,\n",
       "            \u001b[32m'content'\u001b[0m: \u001b[32m'完整微調（Full \u001b[0m\n",
       "\u001b[32mfine-tuning）是最直接的監督微調技術。它涉及使用指令數據集重新訓練預先訓練模型的所有參數。這種方法通常能夠提供最佳結\u001b[0m\n",
       "\u001b[32m果，但需要大量的計算資源（需要多個高端GPU來微調一個8B模型）。由於它修改了整個模型，因此也是最具破壞性的方法，可能會\u001b[0m\n",
       "\u001b[32m導致之前學習的技能和知識的災難性遺忘。\\n\\n低秩適應（Low-Rank \u001b[0m\n",
       "\u001b[32mAdaptation，LoRA）是一種流行的參數高效微調技術。與其重新訓練整個模型，LoRA會凍結權重並在每個目標層引入小型適配器（\u001b[0m\n",
       "\u001b[32m低秩矩陣）。這使得LoRA能夠訓練的參數數量遠遠低於完整的微調（少於1％），從而減少了內存使用和訓練時間。這種方法是非破\u001b[0m\n",
       "\u001b[32m壞性的，因為原始參數被凍結，適配器可以在後續被切換或組合。'\u001b[0m\n",
       "        \u001b[1m}\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mSyntheticData\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mInstruction\u001b[0m=\u001b[32m'什麼是QLoRA，它提供了哪些優勢？'\u001b[0m,\n",
       "        \u001b[33mInput\u001b[0m=\u001b[32m''\u001b[0m,\n",
       "        \u001b[33mOutput\u001b[0m=\u001b[32m''\u001b[0m,\n",
       "        \u001b[33mMetadata\u001b[0m=\u001b[1m{\u001b[0m\n",
       "            \u001b[32m'source'\u001b[0m: \u001b[32m'./example.md'\u001b[0m,\n",
       "            \u001b[32m'content'\u001b[0m: \n",
       "\u001b[32m'QLoRA（量化感知低秩適應）是LoRA的一個擴展，提供了更大的記憶體節省。它提供了高達33%的額外記憶體節省，相比於標準的Lo\u001b[0m\n",
       "\u001b[32mRA，特別是在GPU記憶體受限的情況下。這種增加的效率是以訓練時間更長為代價的，QLoRA通常需要比標準LoRA多39%的時間來訓練\u001b[0m\n",
       "\u001b[32m。雖然QLoRA需要更多的訓練時間，但它在記憶體方面的顯著節省可以使其成為在GPU記憶體有限的場景中唯一可行的選擇。因此，\u001b[0m\n",
       "\u001b[32m這就是我們在下一節中將用來對Llama 3.1 8B模型在Google Colab上進行微調的技術。\\n\\n🦙 對 Llama 3.1 8B \u001b[0m\n",
       "\u001b[32m進行微調\\n\\n為了有效地對 Llama 3.1 8B 模型進行微調，我們將使用 Daniel 和 Michael Han 開發的 Unsloth \u001b[0m\n",
       "\u001b[32m庫。由於其自定義核心，Unsloth 提供了 2 倍的訓練速度和 60% 的記憶體使用率，相比其他選擇更適合在 Colab \u001b[0m\n",
       "\u001b[32m等受限環境中使用。不幸的是，Unsloth 目前只支持單 GPU 設置。對於多 GPU 設置，我推薦流行的替代方案，如 TRL 和 \u001b[0m\n",
       "\u001b[32mAxolotl（兩者都將 Unsloth 作為後端）。'\u001b[0m\n",
       "        \u001b[1m}\u001b[0m\n",
       "    \u001b[1m)\u001b[0m\n",
       "\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "import os\n",
    "import json\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "  base_url = \"http://localhost:11434/v1\",\n",
    "  api_key = \"api_key\"\n",
    ")\n",
    "\n",
    "min_gen=3 # 設定每個文本片段最少產生幾筆合成資料\n",
    "percentage=1 # 設定要產生的合成資料數量是該文本片段總字數的幾％\n",
    "max_retry=5 # 單一個文本片段產生合成資料的過程中最多重試幾次\n",
    "\n",
    "gen_fail=[] # 重試次數超過上限仍失敗的話就紀錄起來並跳過\n",
    "synthetic_dataset=[] # 合成資料集列表\n",
    "\n",
    "count=0\n",
    "gen_try=0\n",
    "for chunk in chunks:    \n",
    "    amount = max(min_gen, int(count_words(chunk.text) * (percentage/100)))\n",
    "\n",
    "    prompt = prompt_template.format(content=chunk.text, amount=amount)\n",
    "\n",
    "    convo = [\n",
    "        {\"role\":\"system\", \"content\":system},\n",
    "        {\"role\":\"user\", \"content\":incontext_user},\n",
    "        {\"role\":\"assistant\", \"content\":incontext_assistant},\n",
    "        {\"role\":\"user\", \"content\":prompt}\n",
    "        ]\n",
    "\n",
    "    while True:\n",
    "        gen_try += 1\n",
    "\n",
    "        completion = client.chat.completions.create(\n",
    "          model=\"llama3.1:latest\",\n",
    "          messages=convo,\n",
    "          temperature=0.9,\n",
    "          top_p=0.7,\n",
    "          max_tokens=2048,\n",
    "          stream=False\n",
    "        )\n",
    "        try: \n",
    "            vaild_response = VaildResponse(vaild_question=completion.choices[0].message.content)\n",
    "            \n",
    "            json_data = json.loads(completion.choices[0].message.content)\n",
    "            #print(f\"=== Vaild response ===\\n{json_data}\\n==========\")\n",
    "            for data in json_data:\n",
    "              metadata = {'source':md_file, 'content': chunk.text}\n",
    "              synthetic_data = SyntheticData(Instruction=data['Instruction'], Metadata=metadata)\n",
    "              synthetic_dataset.append(synthetic_data)\n",
    "\n",
    "              clear_output(wait=True)\n",
    "              count+=1\n",
    "              print(f'Generate instruction {count=}')\n",
    "            break\n",
    "        except (ValidationError, json.JSONDecodeError, KeyError) as ex:\n",
    "          if max_retry < gen_try:\n",
    "              continue\n",
    "          else:\n",
    "              # print(f\"=== Invaild response ===\\n{completion.choices[0].message.content}\\n==========\")\n",
    "              gen_fail.append(completion.choices[0].message.content)\n",
    "              break\n",
    "\n",
    "print(synthetic_dataset[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "system = (\n",
    "    \"\"\"你是一個輔助閱讀的專家. 使用者會提供一段文章及一個問題給你, 你會先閱讀並理解該文章, 並作為一個專家來回答使用者的問題.\n",
    "回答的越仔細越好, 如果你無法從文章內容找到問題的答案, 請直接說你無法找到答案, 絕對不要編造任何回答.\n",
    "回答時必須依據這個格式來提供回答:\n",
    "```\n",
    "{\"Output\": \"你的回答\"}\n",
    "```\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template =(\n",
    "    \"\"\"文章內容:\n",
    "```\n",
    "{content}\n",
    "```\n",
    "問題:\n",
    "```\n",
    "{question}\n",
    "```\n",
    "注意: 不要使用 \"文章中提到...\", \"依據文章內容...\", \"根據文章...\" 以及所有相關的字眼, 請以一個專家的口吻進行回答.\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "incontext_user = (\n",
    "    \"\"\"文章內容:\n",
    "```\n",
    "1. 第一步 - 模型載入：將模型參數移到GPU上。目前的記憶體：模型。\n",
    "2. 第二步 - 前向傳遞：將輸入通過模型，並存儲中間輸出（激活值）。在這一步驟中，存儲這些激活值佔用了記憶體。雖然存儲所有激活值並不是絕對必要的（參見「梯度檢查點」），但對於反向傳播算法的效率而言，存儲它們實際上是必要的。目前的記憶體：模型 +  激活值。\n",
    "3. 第三步 - 反向傳播：從網絡末端計算梯度，並在計算的過程中捨棄前向傳遞的激活值。由於我們已經捨棄了前向傳遞的激活值，在反向傳播過程後，記憶體使用量為模型大小的兩倍 - 一份是權重的拷貝，另一份是梯度的拷貝。目前的記憶體：模型 + 梯度。\n",
    "4. 第四步 - 優化器更新：更新參數，並追蹤優化器的運行參數。許多優化器會追蹤每個模型權重的梯度一階和二階矩的估計值。對於Adam（使用兩個矩），它佔用模型大小的兩倍；對於RMSprop（使用一個矩），它佔用模型大小的一倍；對於SGD（不使用矩），則不佔用模型大小。目前的記憶體：模型 + 梯度 + 梯度矩。\n",
    "5. 第五步 - 執行下一次迭代：在計算了一次梯度並且優化器進行了一步後，梯度和梯度矩仍然存在。因此，在未來的迭代中，您的總最大記憶體使用量將為：模型 + 激活值 + 梯度 + 梯度矩，這意味著記憶體使用量在第二次迭代時會增加，但之後將保持不變。\n",
    "\n",
    "現在，對於通常會消耗記憶體的基本了解後，讓我們來看一些特殊情況和優化方法來節省記憶體。\n",
    "```\n",
    "\n",
    "問題:\n",
    "```\n",
    "優化器更新的過程中，什麼是梯度一階和二階矩的估計值？\n",
    "```\n",
    "注意: 不要使用 \"文章中提到...\", \"依據文章內容...\", \"根據文章...\" 以及所有相關的字眼, 請以一個專家的口吻進行回答.\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "incontext_assistant = (\n",
    "    \"\"\"{\"Output\": \"梯度一階和二階矩的估計值是優化器追蹤模型權重的梯度信息，以便在更新模型參數時使用。具體而言，梯度一階矩是指梯度的均值估計，梯度二階矩是指梯度的平方和估計。這些信息可以幫助優化器更好地調整模型參數，尤其是在模型具有高變異性的情況下。\"}\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Generate output <span style=\"color: #808000; text-decoration-color: #808000\">count</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">81</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Generate output \u001b[33mcount\u001b[0m=\u001b[1;36m81\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">SyntheticData</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">Instruction</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'什麼是監督微調（SFT）的局限性？'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">Input</span>=<span style=\"color: #008000; text-decoration-color: #008000\">''</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">Output</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'監督微調（SFT）的局限性包括在利用基礎模型已有的知識時效果最佳，但學習完全新的信息，例如未知語言，可</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">能會很具挑戰性，並導致更頻繁的幻覺。'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">Metadata</span>=<span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'source'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'./example.md'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'content'</span>: \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'然而，監督微調（SFT）也有其局限性。它在利用基礎模型已有的知識時效果最佳。學習完全新的信息，例如未知語言，可能會很</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">具挑戰性，並導致更頻繁的幻覺。對於基礎模型未知的新領域，建議先在原始數據集上進行持續的預訓練。在另一端，指令模型（</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">即已經微調過的模型）可能已經非常接近您的需求。例如，一個模型可能表現得非常好，但聲稱它是由OpenAI或Meta訓練的，而不</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">是您。在這種情況下，您可能希望通過偏好對齊（preference </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">alignment）稍微調整指令模型的行為。通過提供選擇和拒絕的樣本（約100到1000個樣本）來微調模型的行為，您可以強迫LLM說明</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">您訓練了模型，而不是OpenAI。\\n\\n。⚖️ 監督微調技術（SFT </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Techniques）\\n\\n目前最受歡迎的三種監督微調技術是完整微調（full fine-tuning）、LoRA和QLoRA。'</span>\n",
       "        <span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">SyntheticData</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">Instruction</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'在利用基礎模型已有的知識時，什麼情況下監督微調效果最佳？'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">Input</span>=<span style=\"color: #008000; text-decoration-color: #008000\">''</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">Output</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'在利用基礎模型已有的知識時，監督微調效果最佳。'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">Metadata</span>=<span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'source'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'./example.md'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'content'</span>: \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'然而，監督微調（SFT）也有其局限性。它在利用基礎模型已有的知識時效果最佳。學習完全新的信息，例如未知語言，可能會很</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">具挑戰性，並導致更頻繁的幻覺。對於基礎模型未知的新領域，建議先在原始數據集上進行持續的預訓練。在另一端，指令模型（</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">即已經微調過的模型）可能已經非常接近您的需求。例如，一個模型可能表現得非常好，但聲稱它是由OpenAI或Meta訓練的，而不</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">是您。在這種情況下，您可能希望通過偏好對齊（preference </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">alignment）稍微調整指令模型的行為。通過提供選擇和拒絕的樣本（約100到1000個樣本）來微調模型的行為，您可以強迫LLM說明</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">您訓練了模型，而不是OpenAI。\\n\\n。⚖️ 監督微調技術（SFT </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Techniques）\\n\\n目前最受歡迎的三種監督微調技術是完整微調（full fine-tuning）、LoRA和QLoRA。'</span>\n",
       "        <span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">SyntheticData</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">Instruction</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'如何使用偏好對齊（preference alignment）來微調指令模型的行為？'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">Input</span>=<span style=\"color: #008000; text-decoration-color: #008000\">''</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">Output</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'偏好對齊（preference </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">alignment）是微調指令模型行為的一種方法。它涉及提供選擇和拒絕的樣本（約100到1000個樣本），以強迫LLM說明您訓練了模型</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">，而不是OpenAI或其他公司訓練的模型。在這種情況下，您可以通過微調模型行為，讓它更符合您的偏好和需求。'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">Metadata</span>=<span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'source'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'./example.md'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'content'</span>: \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'然而，監督微調（SFT）也有其局限性。它在利用基礎模型已有的知識時效果最佳。學習完全新的信息，例如未知語言，可能會很</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">具挑戰性，並導致更頻繁的幻覺。對於基礎模型未知的新領域，建議先在原始數據集上進行持續的預訓練。在另一端，指令模型（</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">即已經微調過的模型）可能已經非常接近您的需求。例如，一個模型可能表現得非常好，但聲稱它是由OpenAI或Meta訓練的，而不</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">是您。在這種情況下，您可能希望通過偏好對齊（preference </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">alignment）稍微調整指令模型的行為。通過提供選擇和拒絕的樣本（約100到1000個樣本）來微調模型的行為，您可以強迫LLM說明</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">您訓練了模型，而不是OpenAI。\\n\\n。⚖️ 監督微調技術（SFT </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Techniques）\\n\\n目前最受歡迎的三種監督微調技術是完整微調（full fine-tuning）、LoRA和QLoRA。'</span>\n",
       "        <span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">SyntheticData</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">Instruction</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'什麼是完整微調（Full fine-tuning）？'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">Input</span>=<span style=\"color: #008000; text-decoration-color: #008000\">''</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">Output</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'完整微調是監督微調的一種方法，涉及使用指令數據集重新訓練預先訓練模型的所有參數。'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">Metadata</span>=<span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'source'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'./example.md'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'content'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'完整微調（Full </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">fine-tuning）是最直接的監督微調技術。它涉及使用指令數據集重新訓練預先訓練模型的所有參數。這種方法通常能夠提供最佳結</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">果，但需要大量的計算資源（需要多個高端GPU來微調一個8B模型）。由於它修改了整個模型，因此也是最具破壞性的方法，可能會</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">導致之前學習的技能和知識的災難性遺忘。\\n\\n低秩適應（Low-Rank </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Adaptation，LoRA）是一種流行的參數高效微調技術。與其重新訓練整個模型，LoRA會凍結權重並在每個目標層引入小型適配器（</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">低秩矩陣）。這使得LoRA能夠訓練的參數數量遠遠低於完整的微調（少於1％），從而減少了內存使用和訓練時間。這種方法是非破</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">壞性的，因為原始參數被凍結，適配器可以在後續被切換或組合。'</span>\n",
       "        <span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">SyntheticData</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">Instruction</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'完整微調的優點和缺點是什麼？'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">Input</span>=<span style=\"color: #008000; text-decoration-color: #008000\">''</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">Output</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'完整微調的優點是可以提供最佳結果，但缺點是需要大量的計算資源和修改了整個模型，因此也是最具破壞性的</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">方法，可能會導致之前學習的技能和知識的災難性遺忘。'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">Metadata</span>=<span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'source'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'./example.md'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'content'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'完整微調（Full </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">fine-tuning）是最直接的監督微調技術。它涉及使用指令數據集重新訓練預先訓練模型的所有參數。這種方法通常能夠提供最佳結</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">果，但需要大量的計算資源（需要多個高端GPU來微調一個8B模型）。由於它修改了整個模型，因此也是最具破壞性的方法，可能會</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">導致之前學習的技能和知識的災難性遺忘。\\n\\n低秩適應（Low-Rank </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Adaptation，LoRA）是一種流行的參數高效微調技術。與其重新訓練整個模型，LoRA會凍結權重並在每個目標層引入小型適配器（</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">低秩矩陣）。這使得LoRA能夠訓練的參數數量遠遠低於完整的微調（少於1％），從而減少了內存使用和訓練時間。這種方法是非破</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">壞性的，因為原始參數被凍結，適配器可以在後續被切換或組合。'</span>\n",
       "        <span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">SyntheticData</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">Instruction</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'低秩適應（Low-Rank Adaptation，LoRA）的工作原理是什麼？'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">Input</span>=<span style=\"color: #008000; text-decoration-color: #008000\">''</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">Output</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'低秩適應（Low-Rank </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Adaptation，LoRA）是通過在每個目標層引入小型適配器（低秩矩陣）來微調預先訓練模型的參數。這些適配器可以幫助模型適應</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">新的資料集，而不改變原有的模型參數。這種方法是非破壞性的，因為它們只添加了少量新參數，沒有修改原始的模型權重。'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">Metadata</span>=<span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'source'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'./example.md'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'content'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'完整微調（Full </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">fine-tuning）是最直接的監督微調技術。它涉及使用指令數據集重新訓練預先訓練模型的所有參數。這種方法通常能夠提供最佳結</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">果，但需要大量的計算資源（需要多個高端GPU來微調一個8B模型）。由於它修改了整個模型，因此也是最具破壞性的方法，可能會</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">導致之前學習的技能和知識的災難性遺忘。\\n\\n低秩適應（Low-Rank </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Adaptation，LoRA）是一種流行的參數高效微調技術。與其重新訓練整個模型，LoRA會凍結權重並在每個目標層引入小型適配器（</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">低秩矩陣）。這使得LoRA能夠訓練的參數數量遠遠低於完整的微調（少於1％），從而減少了內存使用和訓練時間。這種方法是非破</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">壞性的，因為原始參數被凍結，適配器可以在後續被切換或組合。'</span>\n",
       "        <span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">SyntheticData</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">Instruction</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'與完整微調相比，低秩適應的計算資源需求如何？'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">Input</span>=<span style=\"color: #008000; text-decoration-color: #008000\">''</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">Output</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'低秩適應比完整微調需要少於1％的計算資源，主要是因為它只在每個目標層引入小型適配器，而不是重新訓練整</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">個模型。這使得它可以有效地降低內存使用和訓練時間的需求。'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">Metadata</span>=<span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'source'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'./example.md'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'content'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'完整微調（Full </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">fine-tuning）是最直接的監督微調技術。它涉及使用指令數據集重新訓練預先訓練模型的所有參數。這種方法通常能夠提供最佳結</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">果，但需要大量的計算資源（需要多個高端GPU來微調一個8B模型）。由於它修改了整個模型，因此也是最具破壞性的方法，可能會</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">導致之前學習的技能和知識的災難性遺忘。\\n\\n低秩適應（Low-Rank </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Adaptation，LoRA）是一種流行的參數高效微調技術。與其重新訓練整個模型，LoRA會凍結權重並在每個目標層引入小型適配器（</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">低秩矩陣）。這使得LoRA能夠訓練的參數數量遠遠低於完整的微調（少於1％），從而減少了內存使用和訓練時間。這種方法是非破</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">壞性的，因為原始參數被凍結，適配器可以在後續被切換或組合。'</span>\n",
       "        <span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">SyntheticData</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">Instruction</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'低秩適應是否是一種破壞性方法？'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">Input</span>=<span style=\"color: #008000; text-decoration-color: #008000\">''</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">Output</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'否，低秩適應是一種非破壞性方法。'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">Metadata</span>=<span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'source'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'./example.md'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'content'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'完整微調（Full </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">fine-tuning）是最直接的監督微調技術。它涉及使用指令數據集重新訓練預先訓練模型的所有參數。這種方法通常能夠提供最佳結</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">果，但需要大量的計算資源（需要多個高端GPU來微調一個8B模型）。由於它修改了整個模型，因此也是最具破壞性的方法，可能會</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">導致之前學習的技能和知識的災難性遺忘。\\n\\n低秩適應（Low-Rank </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Adaptation，LoRA）是一種流行的參數高效微調技術。與其重新訓練整個模型，LoRA會凍結權重並在每個目標層引入小型適配器（</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">低秩矩陣）。這使得LoRA能夠訓練的參數數量遠遠低於完整的微調（少於1％），從而減少了內存使用和訓練時間。這種方法是非破</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">壞性的，因為原始參數被凍結，適配器可以在後續被切換或組合。'</span>\n",
       "        <span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">SyntheticData</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">Instruction</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'完整微調和低秩適應哪種方法能夠提供最佳結果？'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">Input</span>=<span style=\"color: #008000; text-decoration-color: #008000\">''</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">Output</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'完整微調通常能夠提供最佳結果。'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">Metadata</span>=<span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'source'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'./example.md'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'content'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'完整微調（Full </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">fine-tuning）是最直接的監督微調技術。它涉及使用指令數據集重新訓練預先訓練模型的所有參數。這種方法通常能夠提供最佳結</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">果，但需要大量的計算資源（需要多個高端GPU來微調一個8B模型）。由於它修改了整個模型，因此也是最具破壞性的方法，可能會</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">導致之前學習的技能和知識的災難性遺忘。\\n\\n低秩適應（Low-Rank </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Adaptation，LoRA）是一種流行的參數高效微調技術。與其重新訓練整個模型，LoRA會凍結權重並在每個目標層引入小型適配器（</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">低秩矩陣）。這使得LoRA能夠訓練的參數數量遠遠低於完整的微調（少於1％），從而減少了內存使用和訓練時間。這種方法是非破</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">壞性的，因為原始參數被凍結，適配器可以在後續被切換或組合。'</span>\n",
       "        <span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">SyntheticData</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">Instruction</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'什麼是QLoRA，它提供了哪些優勢？'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">Input</span>=<span style=\"color: #008000; text-decoration-color: #008000\">''</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">Output</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'QLoRA是LoRA的一個擴展，提供了更大的記憶體節省。它可以在標準LoRA的基礎上節省高達33%的記憶體，特別是</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">在GPU記憶體受限的情況下。這種增加的效率是以訓練時間更長為代價的，QLoRA通常需要比標準LoRA多39%的時間來訓練。'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">Metadata</span>=<span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'source'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'./example.md'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'content'</span>: \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'QLoRA（量化感知低秩適應）是LoRA的一個擴展，提供了更大的記憶體節省。它提供了高達33%的額外記憶體節省，相比於標準的Lo</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">RA，特別是在GPU記憶體受限的情況下。這種增加的效率是以訓練時間更長為代價的，QLoRA通常需要比標準LoRA多39%的時間來訓練</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">。雖然QLoRA需要更多的訓練時間，但它在記憶體方面的顯著節省可以使其成為在GPU記憶體有限的場景中唯一可行的選擇。因此，</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">這就是我們在下一節中將用來對Llama 3.1 8B模型在Google Colab上進行微調的技術。\\n\\n🦙 對 Llama 3.1 8B </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">進行微調\\n\\n為了有效地對 Llama 3.1 8B 模型進行微調，我們將使用 Daniel 和 Michael Han 開發的 Unsloth </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">庫。由於其自定義核心，Unsloth 提供了 2 倍的訓練速度和 60% 的記憶體使用率，相比其他選擇更適合在 Colab </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">等受限環境中使用。不幸的是，Unsloth 目前只支持單 GPU 設置。對於多 GPU 設置，我推薦流行的替代方案，如 TRL 和 </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Axolotl（兩者都將 Unsloth 作為後端）。'</span>\n",
       "        <span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\n",
       "    \u001b[1;35mSyntheticData\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mInstruction\u001b[0m=\u001b[32m'什麼是監督微調（SFT）的局限性？'\u001b[0m,\n",
       "        \u001b[33mInput\u001b[0m=\u001b[32m''\u001b[0m,\n",
       "        \u001b[33mOutput\u001b[0m=\u001b[32m'監督微調（SFT）的局限性包括在利用基礎模型已有的知識時效果最佳，但學習完全新的信息，例如未知語言，可\u001b[0m\n",
       "\u001b[32m能會很具挑戰性，並導致更頻繁的幻覺。'\u001b[0m,\n",
       "        \u001b[33mMetadata\u001b[0m=\u001b[1m{\u001b[0m\n",
       "            \u001b[32m'source'\u001b[0m: \u001b[32m'./example.md'\u001b[0m,\n",
       "            \u001b[32m'content'\u001b[0m: \n",
       "\u001b[32m'然而，監督微調（SFT）也有其局限性。它在利用基礎模型已有的知識時效果最佳。學習完全新的信息，例如未知語言，可能會很\u001b[0m\n",
       "\u001b[32m具挑戰性，並導致更頻繁的幻覺。對於基礎模型未知的新領域，建議先在原始數據集上進行持續的預訓練。在另一端，指令模型（\u001b[0m\n",
       "\u001b[32m即已經微調過的模型）可能已經非常接近您的需求。例如，一個模型可能表現得非常好，但聲稱它是由OpenAI或Meta訓練的，而不\u001b[0m\n",
       "\u001b[32m是您。在這種情況下，您可能希望通過偏好對齊（preference \u001b[0m\n",
       "\u001b[32malignment）稍微調整指令模型的行為。通過提供選擇和拒絕的樣本（約100到1000個樣本）來微調模型的行為，您可以強迫LLM說明\u001b[0m\n",
       "\u001b[32m您訓練了模型，而不是OpenAI。\\n\\n。⚖️ 監督微調技術（SFT \u001b[0m\n",
       "\u001b[32mTechniques）\\n\\n目前最受歡迎的三種監督微調技術是完整微調（full fine-tuning）、LoRA和QLoRA。'\u001b[0m\n",
       "        \u001b[1m}\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mSyntheticData\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mInstruction\u001b[0m=\u001b[32m'在利用基礎模型已有的知識時，什麼情況下監督微調效果最佳？'\u001b[0m,\n",
       "        \u001b[33mInput\u001b[0m=\u001b[32m''\u001b[0m,\n",
       "        \u001b[33mOutput\u001b[0m=\u001b[32m'在利用基礎模型已有的知識時，監督微調效果最佳。'\u001b[0m,\n",
       "        \u001b[33mMetadata\u001b[0m=\u001b[1m{\u001b[0m\n",
       "            \u001b[32m'source'\u001b[0m: \u001b[32m'./example.md'\u001b[0m,\n",
       "            \u001b[32m'content'\u001b[0m: \n",
       "\u001b[32m'然而，監督微調（SFT）也有其局限性。它在利用基礎模型已有的知識時效果最佳。學習完全新的信息，例如未知語言，可能會很\u001b[0m\n",
       "\u001b[32m具挑戰性，並導致更頻繁的幻覺。對於基礎模型未知的新領域，建議先在原始數據集上進行持續的預訓練。在另一端，指令模型（\u001b[0m\n",
       "\u001b[32m即已經微調過的模型）可能已經非常接近您的需求。例如，一個模型可能表現得非常好，但聲稱它是由OpenAI或Meta訓練的，而不\u001b[0m\n",
       "\u001b[32m是您。在這種情況下，您可能希望通過偏好對齊（preference \u001b[0m\n",
       "\u001b[32malignment）稍微調整指令模型的行為。通過提供選擇和拒絕的樣本（約100到1000個樣本）來微調模型的行為，您可以強迫LLM說明\u001b[0m\n",
       "\u001b[32m您訓練了模型，而不是OpenAI。\\n\\n。⚖️ 監督微調技術（SFT \u001b[0m\n",
       "\u001b[32mTechniques）\\n\\n目前最受歡迎的三種監督微調技術是完整微調（full fine-tuning）、LoRA和QLoRA。'\u001b[0m\n",
       "        \u001b[1m}\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mSyntheticData\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mInstruction\u001b[0m=\u001b[32m'如何使用偏好對齊（preference alignment）來微調指令模型的行為？'\u001b[0m,\n",
       "        \u001b[33mInput\u001b[0m=\u001b[32m''\u001b[0m,\n",
       "        \u001b[33mOutput\u001b[0m=\u001b[32m'偏好對齊（preference \u001b[0m\n",
       "\u001b[32malignment）是微調指令模型行為的一種方法。它涉及提供選擇和拒絕的樣本（約100到1000個樣本），以強迫LLM說明您訓練了模型\u001b[0m\n",
       "\u001b[32m，而不是OpenAI或其他公司訓練的模型。在這種情況下，您可以通過微調模型行為，讓它更符合您的偏好和需求。'\u001b[0m,\n",
       "        \u001b[33mMetadata\u001b[0m=\u001b[1m{\u001b[0m\n",
       "            \u001b[32m'source'\u001b[0m: \u001b[32m'./example.md'\u001b[0m,\n",
       "            \u001b[32m'content'\u001b[0m: \n",
       "\u001b[32m'然而，監督微調（SFT）也有其局限性。它在利用基礎模型已有的知識時效果最佳。學習完全新的信息，例如未知語言，可能會很\u001b[0m\n",
       "\u001b[32m具挑戰性，並導致更頻繁的幻覺。對於基礎模型未知的新領域，建議先在原始數據集上進行持續的預訓練。在另一端，指令模型（\u001b[0m\n",
       "\u001b[32m即已經微調過的模型）可能已經非常接近您的需求。例如，一個模型可能表現得非常好，但聲稱它是由OpenAI或Meta訓練的，而不\u001b[0m\n",
       "\u001b[32m是您。在這種情況下，您可能希望通過偏好對齊（preference \u001b[0m\n",
       "\u001b[32malignment）稍微調整指令模型的行為。通過提供選擇和拒絕的樣本（約100到1000個樣本）來微調模型的行為，您可以強迫LLM說明\u001b[0m\n",
       "\u001b[32m您訓練了模型，而不是OpenAI。\\n\\n。⚖️ 監督微調技術（SFT \u001b[0m\n",
       "\u001b[32mTechniques）\\n\\n目前最受歡迎的三種監督微調技術是完整微調（full fine-tuning）、LoRA和QLoRA。'\u001b[0m\n",
       "        \u001b[1m}\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mSyntheticData\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mInstruction\u001b[0m=\u001b[32m'什麼是完整微調（Full fine-tuning）？'\u001b[0m,\n",
       "        \u001b[33mInput\u001b[0m=\u001b[32m''\u001b[0m,\n",
       "        \u001b[33mOutput\u001b[0m=\u001b[32m'完整微調是監督微調的一種方法，涉及使用指令數據集重新訓練預先訓練模型的所有參數。'\u001b[0m,\n",
       "        \u001b[33mMetadata\u001b[0m=\u001b[1m{\u001b[0m\n",
       "            \u001b[32m'source'\u001b[0m: \u001b[32m'./example.md'\u001b[0m,\n",
       "            \u001b[32m'content'\u001b[0m: \u001b[32m'完整微調（Full \u001b[0m\n",
       "\u001b[32mfine-tuning）是最直接的監督微調技術。它涉及使用指令數據集重新訓練預先訓練模型的所有參數。這種方法通常能夠提供最佳結\u001b[0m\n",
       "\u001b[32m果，但需要大量的計算資源（需要多個高端GPU來微調一個8B模型）。由於它修改了整個模型，因此也是最具破壞性的方法，可能會\u001b[0m\n",
       "\u001b[32m導致之前學習的技能和知識的災難性遺忘。\\n\\n低秩適應（Low-Rank \u001b[0m\n",
       "\u001b[32mAdaptation，LoRA）是一種流行的參數高效微調技術。與其重新訓練整個模型，LoRA會凍結權重並在每個目標層引入小型適配器（\u001b[0m\n",
       "\u001b[32m低秩矩陣）。這使得LoRA能夠訓練的參數數量遠遠低於完整的微調（少於1％），從而減少了內存使用和訓練時間。這種方法是非破\u001b[0m\n",
       "\u001b[32m壞性的，因為原始參數被凍結，適配器可以在後續被切換或組合。'\u001b[0m\n",
       "        \u001b[1m}\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mSyntheticData\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mInstruction\u001b[0m=\u001b[32m'完整微調的優點和缺點是什麼？'\u001b[0m,\n",
       "        \u001b[33mInput\u001b[0m=\u001b[32m''\u001b[0m,\n",
       "        \u001b[33mOutput\u001b[0m=\u001b[32m'完整微調的優點是可以提供最佳結果，但缺點是需要大量的計算資源和修改了整個模型，因此也是最具破壞性的\u001b[0m\n",
       "\u001b[32m方法，可能會導致之前學習的技能和知識的災難性遺忘。'\u001b[0m,\n",
       "        \u001b[33mMetadata\u001b[0m=\u001b[1m{\u001b[0m\n",
       "            \u001b[32m'source'\u001b[0m: \u001b[32m'./example.md'\u001b[0m,\n",
       "            \u001b[32m'content'\u001b[0m: \u001b[32m'完整微調（Full \u001b[0m\n",
       "\u001b[32mfine-tuning）是最直接的監督微調技術。它涉及使用指令數據集重新訓練預先訓練模型的所有參數。這種方法通常能夠提供最佳結\u001b[0m\n",
       "\u001b[32m果，但需要大量的計算資源（需要多個高端GPU來微調一個8B模型）。由於它修改了整個模型，因此也是最具破壞性的方法，可能會\u001b[0m\n",
       "\u001b[32m導致之前學習的技能和知識的災難性遺忘。\\n\\n低秩適應（Low-Rank \u001b[0m\n",
       "\u001b[32mAdaptation，LoRA）是一種流行的參數高效微調技術。與其重新訓練整個模型，LoRA會凍結權重並在每個目標層引入小型適配器（\u001b[0m\n",
       "\u001b[32m低秩矩陣）。這使得LoRA能夠訓練的參數數量遠遠低於完整的微調（少於1％），從而減少了內存使用和訓練時間。這種方法是非破\u001b[0m\n",
       "\u001b[32m壞性的，因為原始參數被凍結，適配器可以在後續被切換或組合。'\u001b[0m\n",
       "        \u001b[1m}\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mSyntheticData\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mInstruction\u001b[0m=\u001b[32m'低秩適應（Low-Rank Adaptation，LoRA）的工作原理是什麼？'\u001b[0m,\n",
       "        \u001b[33mInput\u001b[0m=\u001b[32m''\u001b[0m,\n",
       "        \u001b[33mOutput\u001b[0m=\u001b[32m'低秩適應（Low-Rank \u001b[0m\n",
       "\u001b[32mAdaptation，LoRA）是通過在每個目標層引入小型適配器（低秩矩陣）來微調預先訓練模型的參數。這些適配器可以幫助模型適應\u001b[0m\n",
       "\u001b[32m新的資料集，而不改變原有的模型參數。這種方法是非破壞性的，因為它們只添加了少量新參數，沒有修改原始的模型權重。'\u001b[0m,\n",
       "        \u001b[33mMetadata\u001b[0m=\u001b[1m{\u001b[0m\n",
       "            \u001b[32m'source'\u001b[0m: \u001b[32m'./example.md'\u001b[0m,\n",
       "            \u001b[32m'content'\u001b[0m: \u001b[32m'完整微調（Full \u001b[0m\n",
       "\u001b[32mfine-tuning）是最直接的監督微調技術。它涉及使用指令數據集重新訓練預先訓練模型的所有參數。這種方法通常能夠提供最佳結\u001b[0m\n",
       "\u001b[32m果，但需要大量的計算資源（需要多個高端GPU來微調一個8B模型）。由於它修改了整個模型，因此也是最具破壞性的方法，可能會\u001b[0m\n",
       "\u001b[32m導致之前學習的技能和知識的災難性遺忘。\\n\\n低秩適應（Low-Rank \u001b[0m\n",
       "\u001b[32mAdaptation，LoRA）是一種流行的參數高效微調技術。與其重新訓練整個模型，LoRA會凍結權重並在每個目標層引入小型適配器（\u001b[0m\n",
       "\u001b[32m低秩矩陣）。這使得LoRA能夠訓練的參數數量遠遠低於完整的微調（少於1％），從而減少了內存使用和訓練時間。這種方法是非破\u001b[0m\n",
       "\u001b[32m壞性的，因為原始參數被凍結，適配器可以在後續被切換或組合。'\u001b[0m\n",
       "        \u001b[1m}\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mSyntheticData\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mInstruction\u001b[0m=\u001b[32m'與完整微調相比，低秩適應的計算資源需求如何？'\u001b[0m,\n",
       "        \u001b[33mInput\u001b[0m=\u001b[32m''\u001b[0m,\n",
       "        \u001b[33mOutput\u001b[0m=\u001b[32m'低秩適應比完整微調需要少於1％的計算資源，主要是因為它只在每個目標層引入小型適配器，而不是重新訓練整\u001b[0m\n",
       "\u001b[32m個模型。這使得它可以有效地降低內存使用和訓練時間的需求。'\u001b[0m,\n",
       "        \u001b[33mMetadata\u001b[0m=\u001b[1m{\u001b[0m\n",
       "            \u001b[32m'source'\u001b[0m: \u001b[32m'./example.md'\u001b[0m,\n",
       "            \u001b[32m'content'\u001b[0m: \u001b[32m'完整微調（Full \u001b[0m\n",
       "\u001b[32mfine-tuning）是最直接的監督微調技術。它涉及使用指令數據集重新訓練預先訓練模型的所有參數。這種方法通常能夠提供最佳結\u001b[0m\n",
       "\u001b[32m果，但需要大量的計算資源（需要多個高端GPU來微調一個8B模型）。由於它修改了整個模型，因此也是最具破壞性的方法，可能會\u001b[0m\n",
       "\u001b[32m導致之前學習的技能和知識的災難性遺忘。\\n\\n低秩適應（Low-Rank \u001b[0m\n",
       "\u001b[32mAdaptation，LoRA）是一種流行的參數高效微調技術。與其重新訓練整個模型，LoRA會凍結權重並在每個目標層引入小型適配器（\u001b[0m\n",
       "\u001b[32m低秩矩陣）。這使得LoRA能夠訓練的參數數量遠遠低於完整的微調（少於1％），從而減少了內存使用和訓練時間。這種方法是非破\u001b[0m\n",
       "\u001b[32m壞性的，因為原始參數被凍結，適配器可以在後續被切換或組合。'\u001b[0m\n",
       "        \u001b[1m}\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mSyntheticData\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mInstruction\u001b[0m=\u001b[32m'低秩適應是否是一種破壞性方法？'\u001b[0m,\n",
       "        \u001b[33mInput\u001b[0m=\u001b[32m''\u001b[0m,\n",
       "        \u001b[33mOutput\u001b[0m=\u001b[32m'否，低秩適應是一種非破壞性方法。'\u001b[0m,\n",
       "        \u001b[33mMetadata\u001b[0m=\u001b[1m{\u001b[0m\n",
       "            \u001b[32m'source'\u001b[0m: \u001b[32m'./example.md'\u001b[0m,\n",
       "            \u001b[32m'content'\u001b[0m: \u001b[32m'完整微調（Full \u001b[0m\n",
       "\u001b[32mfine-tuning）是最直接的監督微調技術。它涉及使用指令數據集重新訓練預先訓練模型的所有參數。這種方法通常能夠提供最佳結\u001b[0m\n",
       "\u001b[32m果，但需要大量的計算資源（需要多個高端GPU來微調一個8B模型）。由於它修改了整個模型，因此也是最具破壞性的方法，可能會\u001b[0m\n",
       "\u001b[32m導致之前學習的技能和知識的災難性遺忘。\\n\\n低秩適應（Low-Rank \u001b[0m\n",
       "\u001b[32mAdaptation，LoRA）是一種流行的參數高效微調技術。與其重新訓練整個模型，LoRA會凍結權重並在每個目標層引入小型適配器（\u001b[0m\n",
       "\u001b[32m低秩矩陣）。這使得LoRA能夠訓練的參數數量遠遠低於完整的微調（少於1％），從而減少了內存使用和訓練時間。這種方法是非破\u001b[0m\n",
       "\u001b[32m壞性的，因為原始參數被凍結，適配器可以在後續被切換或組合。'\u001b[0m\n",
       "        \u001b[1m}\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mSyntheticData\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mInstruction\u001b[0m=\u001b[32m'完整微調和低秩適應哪種方法能夠提供最佳結果？'\u001b[0m,\n",
       "        \u001b[33mInput\u001b[0m=\u001b[32m''\u001b[0m,\n",
       "        \u001b[33mOutput\u001b[0m=\u001b[32m'完整微調通常能夠提供最佳結果。'\u001b[0m,\n",
       "        \u001b[33mMetadata\u001b[0m=\u001b[1m{\u001b[0m\n",
       "            \u001b[32m'source'\u001b[0m: \u001b[32m'./example.md'\u001b[0m,\n",
       "            \u001b[32m'content'\u001b[0m: \u001b[32m'完整微調（Full \u001b[0m\n",
       "\u001b[32mfine-tuning）是最直接的監督微調技術。它涉及使用指令數據集重新訓練預先訓練模型的所有參數。這種方法通常能夠提供最佳結\u001b[0m\n",
       "\u001b[32m果，但需要大量的計算資源（需要多個高端GPU來微調一個8B模型）。由於它修改了整個模型，因此也是最具破壞性的方法，可能會\u001b[0m\n",
       "\u001b[32m導致之前學習的技能和知識的災難性遺忘。\\n\\n低秩適應（Low-Rank \u001b[0m\n",
       "\u001b[32mAdaptation，LoRA）是一種流行的參數高效微調技術。與其重新訓練整個模型，LoRA會凍結權重並在每個目標層引入小型適配器（\u001b[0m\n",
       "\u001b[32m低秩矩陣）。這使得LoRA能夠訓練的參數數量遠遠低於完整的微調（少於1％），從而減少了內存使用和訓練時間。這種方法是非破\u001b[0m\n",
       "\u001b[32m壞性的，因為原始參數被凍結，適配器可以在後續被切換或組合。'\u001b[0m\n",
       "        \u001b[1m}\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mSyntheticData\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mInstruction\u001b[0m=\u001b[32m'什麼是QLoRA，它提供了哪些優勢？'\u001b[0m,\n",
       "        \u001b[33mInput\u001b[0m=\u001b[32m''\u001b[0m,\n",
       "        \u001b[33mOutput\u001b[0m=\u001b[32m'QLoRA是LoRA的一個擴展，提供了更大的記憶體節省。它可以在標準LoRA的基礎上節省高達33%的記憶體，特別是\u001b[0m\n",
       "\u001b[32m在GPU記憶體受限的情況下。這種增加的效率是以訓練時間更長為代價的，QLoRA通常需要比標準LoRA多39%的時間來訓練。'\u001b[0m,\n",
       "        \u001b[33mMetadata\u001b[0m=\u001b[1m{\u001b[0m\n",
       "            \u001b[32m'source'\u001b[0m: \u001b[32m'./example.md'\u001b[0m,\n",
       "            \u001b[32m'content'\u001b[0m: \n",
       "\u001b[32m'QLoRA（量化感知低秩適應）是LoRA的一個擴展，提供了更大的記憶體節省。它提供了高達33%的額外記憶體節省，相比於標準的Lo\u001b[0m\n",
       "\u001b[32mRA，特別是在GPU記憶體受限的情況下。這種增加的效率是以訓練時間更長為代價的，QLoRA通常需要比標準LoRA多39%的時間來訓練\u001b[0m\n",
       "\u001b[32m。雖然QLoRA需要更多的訓練時間，但它在記憶體方面的顯著節省可以使其成為在GPU記憶體有限的場景中唯一可行的選擇。因此，\u001b[0m\n",
       "\u001b[32m這就是我們在下一節中將用來對Llama 3.1 8B模型在Google Colab上進行微調的技術。\\n\\n🦙 對 Llama 3.1 8B \u001b[0m\n",
       "\u001b[32m進行微調\\n\\n為了有效地對 Llama 3.1 8B 模型進行微調，我們將使用 Daniel 和 Michael Han 開發的 Unsloth \u001b[0m\n",
       "\u001b[32m庫。由於其自定義核心，Unsloth 提供了 2 倍的訓練速度和 60% 的記憶體使用率，相比其他選擇更適合在 Colab \u001b[0m\n",
       "\u001b[32m等受限環境中使用。不幸的是，Unsloth 目前只支持單 GPU 設置。對於多 GPU 設置，我推薦流行的替代方案，如 TRL 和 \u001b[0m\n",
       "\u001b[32mAxolotl（兩者都將 Unsloth 作為後端）。'\u001b[0m\n",
       "        \u001b[1m}\u001b[0m\n",
       "    \u001b[1m)\u001b[0m\n",
       "\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "max_retry=10\n",
    "output_gen_fail=[]\n",
    "\n",
    "count=0\n",
    "for data in synthetic_dataset:\n",
    "    clear_output(wait=True)\n",
    "    try_gen=0\n",
    "    prompt = prompt_template.format(content=data.Metadata['content'], question=data.Instruction)\n",
    "\n",
    "    convo = [\n",
    "        {\"role\":\"system\", \"content\":system},\n",
    "        {\"role\":\"user\", \"content\":incontext_user},\n",
    "        {\"role\":\"assistant\", \"content\":incontext_assistant},\n",
    "        {\"role\":\"user\", \"content\":prompt}\n",
    "        ]\n",
    "    \n",
    "    while True:\n",
    "        gen_try += 1\n",
    "\n",
    "        completion = client.chat.completions.create(\n",
    "          model=\"llama3.1:latest\",\n",
    "          messages=convo,\n",
    "          temperature=0.4,\n",
    "          top_p=0.7,\n",
    "          max_tokens=1024,\n",
    "          stream=False\n",
    "        )\n",
    "        try: \n",
    "            vaild_response = VaildResponse(vaild_question=completion.choices[0].message.content)\n",
    "            \n",
    "            json_data = json.loads(completion.choices[0].message.content)\n",
    "            # print(f\"=== Vaild response ===\\n{json_data}\\n==========\")\n",
    "            if len(json_data) > 1:\n",
    "                raise ValueError\n",
    "            \n",
    "            count+=1\n",
    "            print(f'Generate output {count=}')\n",
    "            data.Output=json_data['Output']\n",
    "            break\n",
    "        except (ValidationError, json.JSONDecodeError, KeyError, ValueError) as ex:\n",
    "          if max_retry < gen_try:\n",
    "              continue\n",
    "          else:\n",
    "              # print(f\"=== Invaild response ===\\n{completion.choices[0].message.content}\\n==========\")\n",
    "              output_gen_fail.append(completion.choices[0].message.content)\n",
    "              break\n",
    "\n",
    "print(synthetic_dataset[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "完成後可以存成 `Dataset`格式用來分享到HuggingFace, 也可以存成 `Dataframe` 做其他處理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "\n",
    "data_dict_list = [data.model_dump() for data in synthetic_dataset]\n",
    "\n",
    "dataset = Dataset.from_list(data_dict_list)\n",
    "\n",
    "# save_dir = f\"{datetime.now():%Y%m%d%H%M%S}\"\n",
    "# os.makedirs(save_dir, exist_ok=True)\n",
    "# dataset.save_to_disk(save_dir)\n",
    "\n",
    "df = pd.DataFrame(data_dict_list)\n",
    "\n",
    "# csv_file=os.path.join(save_dir, f\"{save_dir}_SyntheticDataset.csv\")\n",
    "# df.to_csv(csv_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Instruction</th>\n",
       "      <th>Input</th>\n",
       "      <th>Output</th>\n",
       "      <th>Metadata</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>什麼是監督微調（SFT）的局限性？</td>\n",
       "      <td></td>\n",
       "      <td>監督微調（SFT）的局限性包括在利用基礎模型已有的知識時效果最佳，但學習完全新的信息，例如未...</td>\n",
       "      <td>{'source': './example.md', 'content': '然而，監督微調...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>在利用基礎模型已有的知識時，什麼情況下監督微調效果最佳？</td>\n",
       "      <td></td>\n",
       "      <td>在利用基礎模型已有的知識時，監督微調效果最佳。</td>\n",
       "      <td>{'source': './example.md', 'content': '然而，監督微調...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>如何使用偏好對齊（preference alignment）來微調指令模型的行為？</td>\n",
       "      <td></td>\n",
       "      <td>偏好對齊（preference alignment）是微調指令模型行為的一種方法。它涉及提供...</td>\n",
       "      <td>{'source': './example.md', 'content': '然而，監督微調...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>什麼是完整微調（Full fine-tuning）？</td>\n",
       "      <td></td>\n",
       "      <td>完整微調是監督微調的一種方法，涉及使用指令數據集重新訓練預先訓練模型的所有參數。</td>\n",
       "      <td>{'source': './example.md', 'content': '完整微調（Fu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>完整微調的優點和缺點是什麼？</td>\n",
       "      <td></td>\n",
       "      <td>完整微調的優點是可以提供最佳結果，但缺點是需要大量的計算資源和修改了整個模型，因此也是最具破...</td>\n",
       "      <td>{'source': './example.md', 'content': '完整微調（Fu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>低秩適應（Low-Rank Adaptation，LoRA）的工作原理是什麼？</td>\n",
       "      <td></td>\n",
       "      <td>低秩適應（Low-Rank Adaptation，LoRA）是通過在每個目標層引入小型適配器...</td>\n",
       "      <td>{'source': './example.md', 'content': '完整微調（Fu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>與完整微調相比，低秩適應的計算資源需求如何？</td>\n",
       "      <td></td>\n",
       "      <td>低秩適應比完整微調需要少於1％的計算資源，主要是因為它只在每個目標層引入小型適配器，而不是重...</td>\n",
       "      <td>{'source': './example.md', 'content': '完整微調（Fu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>低秩適應是否是一種破壞性方法？</td>\n",
       "      <td></td>\n",
       "      <td>否，低秩適應是一種非破壞性方法。</td>\n",
       "      <td>{'source': './example.md', 'content': '完整微調（Fu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>完整微調和低秩適應哪種方法能夠提供最佳結果？</td>\n",
       "      <td></td>\n",
       "      <td>完整微調通常能夠提供最佳結果。</td>\n",
       "      <td>{'source': './example.md', 'content': '完整微調（Fu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>什麼是QLoRA，它提供了哪些優勢？</td>\n",
       "      <td></td>\n",
       "      <td>QLoRA是LoRA的一個擴展，提供了更大的記憶體節省。它可以在標準LoRA的基礎上節省高達...</td>\n",
       "      <td>{'source': './example.md', 'content': 'QLoRA（量...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Instruction Input  \\\n",
       "0                          什麼是監督微調（SFT）的局限性？         \n",
       "1               在利用基礎模型已有的知識時，什麼情況下監督微調效果最佳？         \n",
       "2  如何使用偏好對齊（preference alignment）來微調指令模型的行為？         \n",
       "3                 什麼是完整微調（Full fine-tuning）？         \n",
       "4                             完整微調的優點和缺點是什麼？         \n",
       "5    低秩適應（Low-Rank Adaptation，LoRA）的工作原理是什麼？         \n",
       "6                     與完整微調相比，低秩適應的計算資源需求如何？         \n",
       "7                            低秩適應是否是一種破壞性方法？         \n",
       "8                     完整微調和低秩適應哪種方法能夠提供最佳結果？         \n",
       "9                         什麼是QLoRA，它提供了哪些優勢？         \n",
       "\n",
       "                                              Output  \\\n",
       "0  監督微調（SFT）的局限性包括在利用基礎模型已有的知識時效果最佳，但學習完全新的信息，例如未...   \n",
       "1                            在利用基礎模型已有的知識時，監督微調效果最佳。   \n",
       "2  偏好對齊（preference alignment）是微調指令模型行為的一種方法。它涉及提供...   \n",
       "3           完整微調是監督微調的一種方法，涉及使用指令數據集重新訓練預先訓練模型的所有參數。   \n",
       "4  完整微調的優點是可以提供最佳結果，但缺點是需要大量的計算資源和修改了整個模型，因此也是最具破...   \n",
       "5  低秩適應（Low-Rank Adaptation，LoRA）是通過在每個目標層引入小型適配器...   \n",
       "6  低秩適應比完整微調需要少於1％的計算資源，主要是因為它只在每個目標層引入小型適配器，而不是重...   \n",
       "7                                   否，低秩適應是一種非破壞性方法。   \n",
       "8                                    完整微調通常能夠提供最佳結果。   \n",
       "9  QLoRA是LoRA的一個擴展，提供了更大的記憶體節省。它可以在標準LoRA的基礎上節省高達...   \n",
       "\n",
       "                                            Metadata  \n",
       "0  {'source': './example.md', 'content': '然而，監督微調...  \n",
       "1  {'source': './example.md', 'content': '然而，監督微調...  \n",
       "2  {'source': './example.md', 'content': '然而，監督微調...  \n",
       "3  {'source': './example.md', 'content': '完整微調（Fu...  \n",
       "4  {'source': './example.md', 'content': '完整微調（Fu...  \n",
       "5  {'source': './example.md', 'content': '完整微調（Fu...  \n",
       "6  {'source': './example.md', 'content': '完整微調（Fu...  \n",
       "7  {'source': './example.md', 'content': '完整微調（Fu...  \n",
       "8  {'source': './example.md', 'content': '完整微調（Fu...  \n",
       "9  {'source': './example.md', 'content': 'QLoRA（量...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next step\n",
    "\n",
    "資料集生成了, 接下來可以做什麼？\n",
    "\n",
    "- 這些資料集真的夠好嗎？  \n",
    "  [Textbooks Are All You Need](https://arxiv.org/abs/2306.11644) 中提到高精度的資料對模型的成效是有很關鍵的影響的.  \n",
    "  可以嘗試用 Nvidia 推出的 [nemotron-4-340b-instruct](https://build.nvidia.com/nvidia/nemotron-4-340b-instruct) 來篩選出優質的資料, 但是目前並未對中文(尤其是繁體中文)做過優化, 成效可能不佳.  \n",
    "  也可以試看看開源的 [internlm/internlm2-20b-reward](https://huggingface.co/internlm/internlm2-20b-reward), 是個對中文有做過優化的Reward模型.\n",
    "\n",
    "- 調整提示詞看看會發生什麼事  \n",
    "  本文使用到的提示詞都是我自己測試時使用的提示詞, 我在測試過程中發現可能只加一兩個字就會得到非常不同的結果！  \n",
    "  例如:\n",
    "  - prompt: \"...依據文章產生一個問題\", LLM: \"問題：什麼是梯度檢查點，為什麼存儲所有激活值不是絕對必要的？\"\n",
    "  - prompt: \"...依據文章產生一個讓人想笑的問題\", LLM: 問題：\"如果模型在跑了5步後突然開始思考自己的存在意義，是否會出現\"存在危機\"，導致記憶體使用量增加，然後需要進行優化更新？😂\"  (emoji也是LLM給的)\n",
    "  \n",
    "- 換個模型試試吧！  \n",
    "  不同的模型產生出來的結果都不一樣, 有趣的同時還可以增加資料量！\n",
    "\n",
    "- Fine-tune你專屬的模型吧！  \n",
    "  本文中使用到的示範文件就是我自己<s>丟給AI</s>將這篇[Fine-tune Llama 3.1 Ultra-Efficiently with Unsloth](https://huggingface.co/blog/mlabonne/sft-llama3)翻成中文的, 文中說明了如何使用Unsloth微調LLM.\n",
    "\n",
    "- 插一手  \n",
    "  藉由 [Human in the loop(HITL)](https://cloud.google.com/discover/human-in-the-loop)的方式, 人類跟AI協作, 在數據生成的過程中加入人類的輔助, 提升數據品質!\n",
    "\n",
    "- 優化程式碼  \n",
    "  本文中提供的程式碼都盡量精簡, 還有很多地方可以在優化, 做更細節的處理, 靠你了🫠"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
